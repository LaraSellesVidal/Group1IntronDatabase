{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "import os\n",
    "import certifi\n",
    "import pickle\n",
    "from Bio import Entrez, SeqIO, AlignIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.SeqFeature import CompoundLocation, ExactPosition, BeforePosition, AfterPosition\n",
    "from io import StringIO\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import string\n",
    "from ete3 import NCBITaxa\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from Bio.Data import CodonTable\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import sys\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we run Infernal's cmsearch of the group I intron covariance model (obtained from Rfam) against the entire NT database. We use the parameters given in Rfam for cmsearch, but changing the database size in Mbp to the size of NT (1.3 trillion bases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables\n",
    "num_cpus = 100\n",
    "output_file_path = \"/path/to/output/tbl/out_file.tbl\"\n",
    "cm_model_path = \"/path/to/cm/model.cm\"\n",
    "input_fasta_file_path = \"/path/to/nt/database/nt\"\n",
    "path_to_cmsearch = \"/path/to/cmsearch/bin/cmsearch\"\n",
    "\n",
    "# Define the command\n",
    "command = f\"{path_to_cmsearch} --cpu {num_cpus} --verbose --nohmmonly -E 1000 -Z 1300000 --tblout {output_file_path} {cm_model_path} {input_fasta_file_path}\"\n",
    "\n",
    "# Run the command\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then read the output of cmsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_infernal_output(file_path):\n",
    "    \"\"\"Read an Infernal output file into a pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            fields = line.split(maxsplit=17)\n",
    "            data.append(fields)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = ['target name', 'accession', 'query name', 'accession', 'mdl', 'mdl from', 'mdl to', 'seq from', 'seq to', 'strand', 'trunc', 'pass', 'gc', 'bias', 'score', 'E-value', 'inc', 'description of target']\n",
    "    return df\n",
    "\n",
    "infernal_NT_search_path = output_file_path  # Path to the Infernal output file\n",
    "\n",
    "infernal_NT_search_all_hits = read_infernal_output(infernal_NT_search_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to fetch the GenBank entries of all records that gave a cmsearch hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "\n",
    "email = \"your.email@email.com\"  # Your email\n",
    "\n",
    "def fetch_genbank_records_batch(email, batch_ids, max_tries=100):\n",
    "    \"\"\"Fetch GenBank records for a batch of IDs.\"\"\"\n",
    "    Entrez.email = email\n",
    "    records = {}\n",
    "    for _ in range(max_tries):\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"nucleotide\", id=batch_ids, rettype=\"gb\", retmode=\"text\")\n",
    "            for record in SeqIO.parse(handle, \"genbank\"):\n",
    "                records[record.id] = record\n",
    "            handle.close()\n",
    "            return records\n",
    "        except:\n",
    "            time.sleep(1)  # Wait for a second before retrying\n",
    "\n",
    "def fetch_genbank_records(email, ids, batch_size=500, max_workers=10):\n",
    "    \"\"\"Fetch GenBank records for a list of IDs in batches using multiple threads.\"\"\"\n",
    "    records = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(fetch_genbank_records_batch, email, ids[i:i+batch_size]) for i in range(0, len(ids), batch_size)}\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures), 1):\n",
    "            records.update(future.result())  # Update the records dictionary with the result of the future\n",
    "            if i % 50 == 0:  # Every 50 batches\n",
    "                with open('genbank_records.pkl', 'wb') as f:\n",
    "                    pickle.dump(records, f)  # Save the records to a file, to be able to restore partial results\n",
    "                print(f\"Saved results after processing {i} batches\")\n",
    "    return records\n",
    "\n",
    "ids = list(set(infernal_NT_search_all_hits[\"target name\"].tolist()))  # Remove duplicates before passing to function\n",
    "\n",
    "infernal_NT_search_genbank_recs = fetch_genbank_records(email, ids, batch_size=50, max_workers=10)\n",
    "\n",
    "# save the final results \n",
    "with open('genbank_records.pkl', 'wb') as f:\n",
    "    pickle.dump(infernal_NT_search_genbank_recs, f) # note this will require large amount of memory to write and also to read later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('genbank_records.pkl', 'rb') as f:\n",
    "    infernal_NT_search_genbank_recs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some keys will have records with undefined sequences because their sequences are too large and must be explicitly requested as fasta. So we find these and handle them separately. We save these separately as they are very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genbank_recs_with_undefined_seqs = [key for key, value in infernal_NT_search_genbank_recs.items() if value.seq.defined is False]\n",
    "\n",
    "batch_size = 100\n",
    "genbank_ids_to_refetch = genbank_recs_with_undefined_seqs\n",
    "seqs_of_records_with_undefined_seqs = {}\n",
    "\n",
    "for i in range(0, len(genbank_ids_to_refetch), batch_size):\n",
    "    print(f\"Processing batch {i+1}-{i+batch_size}\")\n",
    "    batch_ids = genbank_ids_to_refetch[i:i+batch_size]\n",
    "    handle = Entrez.efetch(db=\"nucleotide\", id=batch_ids, rettype=\"fasta\", retmode=\"text\")\n",
    "    records = list(SeqIO.parse(handle, \"fasta\"))  # Convert iterator to list\n",
    "    with open(f'records_with_previously_undefined_seqs_batch_{i+1}_{i+batch_size}.fasta', 'w') as f:\n",
    "        SeqIO.write(records, f, \"fasta\")  # Write records to file\n",
    "    for record in records:\n",
    "        seqs_of_records_with_undefined_seqs[record.id] = record.seq\n",
    "    print(f\"Finished processing batch {i+1}-{i+batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If required we can restore the full sequences from the fasta files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_of_records_with_undefined_seqs = {}\n",
    "merged_records = []\n",
    "\n",
    "# Find all files that match the pattern\n",
    "for file_name in glob.glob(\"records_with_previously_undefined_seqs_batch_*.fasta\"):\n",
    "    # Open the file and parse the records\n",
    "    with open(file_name, \"r\") as handle:\n",
    "        records = list(SeqIO.parse(handle, \"fasta\"))\n",
    "    # Add the records to the merged_records list\n",
    "    merged_records.extend(records)\n",
    "\n",
    "# Add the sequences to the seqs_of_records_with_undefined_seqs dictionary\n",
    "for record in merged_records:\n",
    "    seqs_of_records_with_undefined_seqs[record.id] = record.seq\n",
    "\n",
    "len(seqs_of_records_with_undefined_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to filter the intron hits to keep only those for which we can find reliable boundaries.\n",
    "In order to do so, we look for features labeled as intron in the corresponding GenBank entries that overlap by a minimum overlap threshold with the cmsearch hit.\n",
    "If there are no such features, we look for features with a location of type CompoundLocation (those shown as join(...)) and look for a gap between exons that overlaps with the cmsearch hit.\n",
    "We perform the search in both the direct and the complementary strand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_threshold = 50\n",
    "intron_boundaries_genbank_infernal_NT_search = {}\n",
    "rows_with_undefined_seqs = []\n",
    "\n",
    "for index, row in tqdm(infernal_NT_search_all_hits.iterrows(), total=infernal_NT_search_all_hits.shape[0]):\n",
    "    infernal_intron_start = int(row[\"seq from\"])\n",
    "    infernal_intron_end = int(row[\"seq to\"])\n",
    "    infernal_boundaries = (infernal_intron_start, infernal_intron_end)\n",
    "    infernal_hit_strand = row[\"strand\"]\n",
    "    genbank_ID = row[\"target name\"]\n",
    "    if genbank_ID == 'MT229979.1':\n",
    "        # skip this iteration since this seems to be a strange record that was removed from GenBank\n",
    "        continue\n",
    "    genbank_record = infernal_NT_search_genbank_recs[genbank_ID]\n",
    "    if genbank_record.seq.defined:\n",
    "        genbank_sequence_raw = genbank_record.seq\n",
    "    else:\n",
    "        genbank_sequence_raw = seqs_of_records_with_undefined_seqs[genbank_ID]\n",
    "    genbank_sequence = genbank_sequence_raw.replace('U', 'T')\n",
    "    genbank_features = genbank_record.features \n",
    "    new_intron_boundaries = None\n",
    "    source = None\n",
    "    candidate_reliable_introns = []\n",
    "    if(infernal_intron_start < infernal_intron_end):\n",
    "        infernal_intron_range = range(infernal_intron_start, infernal_intron_end)\n",
    "        for feature in genbank_features:\n",
    "            if feature.type == 'intron':\n",
    "            # Check for 'intron' features\n",
    "                if isinstance(feature.location.start, ExactPosition) and isinstance(feature.location.end, ExactPosition):\n",
    "                    candidate_intron_range = range(int(feature.location.start), int(feature.location.end))\n",
    "                    overlap = len(set(infernal_intron_range) & set(candidate_intron_range))\n",
    "                    if overlap >= overlap_threshold and candidate_intron_range[0] >= 1 and candidate_intron_range[1] < (len(genbank_sequence) - 1):\n",
    "                        source = 'intron'\n",
    "                        new_intron_start = int(feature.location.start) - 1\n",
    "                        new_intron_end = int(feature.location.end)\n",
    "                        candidate_intron_sequence = genbank_sequence[new_intron_start:(new_intron_end + 1)]\n",
    "                        candidate_reliable_introns.append((genbank_ID, new_intron_start, new_intron_end, candidate_intron_sequence, overlap, source))\n",
    "            elif isinstance(feature.location, CompoundLocation):\n",
    "            # Check for 'compound location' features\n",
    "                for i in range(len(feature.location.parts) - 1):\n",
    "                    # Get the end of the first part and the start of the second part\n",
    "                    end_first_part = feature.location.parts[i].end\n",
    "                    start_second_part = feature.location.parts[i+1].start\n",
    "                    candidate_intron_range = range(int(end_first_part), int(start_second_part))\n",
    "                    overlap = len(set(infernal_intron_range) & set(candidate_intron_range))\n",
    "                    if overlap >= overlap_threshold and start_second_part - end_first_part < 3000:\n",
    "                        source = 'compoundLocation'\n",
    "                        #if(key == 'AY518280.1'):\n",
    "                        #    breakpoint()\n",
    "                        new_intron_start = end_first_part - 1\n",
    "                        new_intron_end = start_second_part\n",
    "                        candidate_intron_sequence = genbank_sequence[new_intron_start:(new_intron_end + 1)]\n",
    "                        candidate_reliable_introns.append((genbank_ID, new_intron_start, new_intron_end, candidate_intron_sequence, overlap, source))\n",
    "    else:\n",
    "        infernal_intron_range = range(infernal_intron_start, infernal_intron_end, -1)\n",
    "        for feature in genbank_features:\n",
    "            if feature.type == 'intron' and feature.location.strand == -1:\n",
    "            # Check for 'intron' features\n",
    "                if isinstance(feature.location.start, ExactPosition) and isinstance(feature.location.end, ExactPosition):\n",
    "                    candidate_intron_range = range(int(feature.location.end), int(feature.location.start), -1)\n",
    "                    overlap = len(set(infernal_intron_range) & set(candidate_intron_range))\n",
    "                    # It seems that ALWAYS int(feature.location.end) > int(feature.location.start) in these cases of features of type intron in strand -1\n",
    "                    if overlap >= overlap_threshold and candidate_intron_range[1] >= 1 and candidate_intron_range[0] < (len(genbank_sequence) - 1): #note indexes for accession on the range are reversed\n",
    "                        source = 'intron_reverseStrand'\n",
    "                        new_intron_start = int(feature.location.start) -1\n",
    "                        new_intron_end = int(feature.location.end)\n",
    "                        candidate_intron_sequence = genbank_sequence[new_intron_start:(new_intron_end + 1)]\n",
    "                        # Reverse complement the candidate_intron_sequence\n",
    "                        reverse_complement_sequence = Seq(candidate_intron_sequence).reverse_complement()\n",
    "                        candidate_reliable_introns.append((genbank_ID, new_intron_start, new_intron_end, reverse_complement_sequence, overlap, source))\n",
    "            elif isinstance(feature.location, CompoundLocation) and feature.location.strand == -1:\n",
    "                for i in range(len(feature.location.parts) - 1):\n",
    "                    start_first_part = feature.location.parts[i].start\n",
    "                    end_second_part = feature.location.parts[i+1].end\n",
    "                    candidate_intron_range = range(int(start_first_part), int(end_second_part), -1)\n",
    "                    overlap = len(set(infernal_intron_range) & set(candidate_intron_range))\n",
    "                    if overlap >= overlap_threshold and start_first_part - end_second_part < 3000:\n",
    "                        source = 'compoundLocation_reverseStrand'\n",
    "                        new_intron_start = end_second_part\n",
    "                        new_intron_end = start_first_part\n",
    "                        candidate_intron_sequence = genbank_sequence[new_intron_start:(new_intron_end + 1)]\n",
    "                        reverse_complement_sequence = Seq(candidate_intron_sequence).reverse_complement()\n",
    "                        candidate_reliable_introns.append((genbank_ID, new_intron_start, new_intron_end, reverse_complement_sequence, overlap, source))\n",
    "    if len(candidate_reliable_introns) >= 1:\n",
    "        if(len(candidate_reliable_introns) == 1):\n",
    "            selected_intron = candidate_reliable_introns[0]\n",
    "        else:\n",
    "            # Sort the candidate introns by overlap in descending order\n",
    "            candidate_reliable_introns.sort(key=lambda x: x[4], reverse=True)\n",
    "            # Get the highest overlap value\n",
    "            highest_overlap = candidate_reliable_introns[0][4]\n",
    "            # Filter the candidate introns with the highest overlap value\n",
    "            highest_overlap_introns = [intron for intron in candidate_reliable_introns if intron[4] == highest_overlap]\n",
    "            # Check if all the sequences are the same\n",
    "            sequences = [intron[3] for intron in highest_overlap_introns]\n",
    "            if len(set(sequences)) == 1:\n",
    "                selected_intron = highest_overlap_introns[0]\n",
    "            else:\n",
    "                # Check which introns have 'T' as their first letter\n",
    "                introns_with_T = [intron for intron in highest_overlap_introns if intron[3][0] == 'T']\n",
    "                if len(introns_with_T) > 0:\n",
    "                    selected_intron = introns_with_T[0]\n",
    "                else:\n",
    "                    selected_intron = highest_overlap_introns[0]\n",
    "    else:\n",
    "        selected_intron = None\n",
    "    if genbank_ID in intron_boundaries_genbank_infernal_NT_search:\n",
    "        if selected_intron is not None:\n",
    "            intron_boundaries_genbank_infernal_NT_search[genbank_ID] = [intron_boundaries_genbank_infernal_NT_search[genbank_ID], selected_intron]\n",
    "    else:\n",
    "        intron_boundaries_genbank_infernal_NT_search[genbank_ID] = selected_intron\n",
    "\n",
    "\n",
    "intron_boundaries_genbank_infernal_NT_search_found = {key: value for key, value in intron_boundaries_genbank_infernal_NT_search.items() if value is not None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select only introns that start with T (keeping in mind that the first nucleotide of each extracted intron is the last nucleotide of the exon), and we add the sequence length to each intron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT = {}\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found.items():\n",
    "    if isinstance(value, list):\n",
    "        # Filter out introns that don't start with a \"T\"\n",
    "        good_introns = [intron for intron in value if intron is not None and intron[3].startswith('T')]\n",
    "        if good_introns:\n",
    "            # If there's only one good intron, store it as a tuple instead of a list\n",
    "            if len(good_introns) == 1:\n",
    "                intron_boundaries_genbank_infernal_NT_search_found_startsWithT[key] = good_introns[0]\n",
    "            else:\n",
    "                intron_boundaries_genbank_infernal_NT_search_found_startsWithT[key] = good_introns\n",
    "    else:\n",
    "        # If the intron doesn't start with a \"T\", skip it\n",
    "        if not value[3].startswith('T'):\n",
    "            continue\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT[key] = value\n",
    "\n",
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths = {}\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found_startsWithT.items():\n",
    "    if isinstance(value, list):\n",
    "        new_value = []\n",
    "        for intron in value:\n",
    "            intron_length = len(intron[3])\n",
    "            new_intron = intron + (intron_length,)\n",
    "            new_value.append(new_intron)\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths[key] = new_value\n",
    "    else:\n",
    "        intron_length = len(value[3])\n",
    "        new_value = value + (intron_length,)\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths[key] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, there seems to be a single abnormal occurrence of an intron too long due to it having a compound feature annotated with a gap of nearly 100000 nucleotides. We simply remove this instance with a very high max length threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the maximum length\n",
    "max_length = 10000\n",
    "\n",
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs = {}\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths.items():\n",
    "    if isinstance(value, list):\n",
    "        # Filter out introns that are too long\n",
    "        good_introns = [intron for intron in value if intron[-1] <= max_length]\n",
    "        if good_introns:\n",
    "            # If there's only one good intron, store it as a tuple instead of a list\n",
    "            if len(good_introns) == 1:\n",
    "                intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs[key] = good_introns[0]\n",
    "            else:\n",
    "                intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs[key] = good_introns\n",
    "    else:\n",
    "        # If the intron is too long, skip it\n",
    "        if value[-1] > max_length:\n",
    "            continue\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the structure that we have is a dictionary where each key is a GenBank entry that contains at least 1 group I intron with reliable boundaries. If there is a single such intron, the value is a tuple describing the intron. If there are multiple, the value is a list of such tuples. We now want to extend each intron tuple with additional metadata: organism name, nucleic acid molecular type, taxonomy ID and subcellular location (organelle). We can extract these from the source feature of each GenBank entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend = {}\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs.items():\n",
    "    # Extract the additional data from the 'source' feature\n",
    "    source_feature = next(feature for feature in infernal_NT_search_genbank_recs[key].features if feature.type == 'source')\n",
    "    organism = source_feature.qualifiers.get('organism', [''])[0]\n",
    "    mol_type = source_feature.qualifiers.get('mol_type', [''])[0]\n",
    "    db_xref = source_feature.qualifiers.get('db_xref', [''])\n",
    "    # db_xref might be a list, so we want to take the element that contains the substring taxon\n",
    "    if isinstance(db_xref, list):\n",
    "        db_xref = next(xref for xref in db_xref if 'taxon' in xref)\n",
    "    organelle = source_feature.qualifiers.get('organelle', [''])[0]\n",
    "    if isinstance(value, list):\n",
    "        new_value = []\n",
    "        for intron in value:\n",
    "            new_intron = intron + (organism, mol_type, db_xref, organelle)\n",
    "            new_value.append(new_intron)\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend[key] = new_value\n",
    "    else:\n",
    "        new_value = value + (organism, mol_type, db_xref, organelle)\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend[key] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now flatten the dictionary so that each entry has a tuple as value. When required, we append _i to the keys with more than 1 intron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened = {}\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend.items():\n",
    "    if isinstance(value, list):\n",
    "        for i, intron in enumerate(value, start=1):\n",
    "            new_key = f\"{key}_{i}\"\n",
    "            intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened[new_key] = intron\n",
    "    else:\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the 43913 group I introns with reliable boundaries. We can save the dictionary for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened.pkl', 'wb') as f:\n",
    "    pickle.dump(intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened.pkl', 'rb') as f:\n",
    "    intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each value in the dictionary of introns is a tuple describing the intron. We convert these to dictionaries to make clearer what each element is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict = {}\n",
    "\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened.items():\n",
    "    genbank_id, start_position, end_position, sequence, overlap, boundariesSource, length, organism, molecule_type, tax_id, organelle = value\n",
    "    entry_dict = {\n",
    "        'GenBankID': genbank_id,\n",
    "        'startPosition': start_position,\n",
    "        'endPosition': end_position,\n",
    "        'sequence': sequence,\n",
    "        'overlapWithInfernalHit': overlap,\n",
    "        'boundariesSource': boundariesSource,\n",
    "        'length': length,\n",
    "        'organism': organism,\n",
    "        'moleculeType': molecule_type,\n",
    "        'taxID': tax_id,\n",
    "        'organelle': organelle\n",
    "    }\n",
    "    intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict[key] = entry_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through manual inspection of entries with strange taxonomy, we found 3 outliers that require manual fixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tFirstly, there is a case where authors seem to have by mistake assigned taxid of Olea gastropod genus,\n",
    "# whereas it should be the olive Olea as indicated by the organism name. So we are going to fix it manually\n",
    "# the corrected data are taken from Table S1 of the corresponding paper\n",
    "MT560017_1_entry = intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict['MT560017.1']\n",
    "MT560017_1_entry[\"organism\"] = \"Olea sp. POC544315\"\n",
    "MT560017_1_entry[\"taxID\"] = \"taxon:2813885\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we need to fix 2 entries that are annotated as artificial DNA because they were deposited as clones into vectors\n",
    "# First entry for K03428.1\n",
    "K03428_1_entry = intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict['K03428.1']\n",
    "K03428_1_entry[\"organism\"] = \"Tetrahymena thermophila\"\n",
    "K03428_1_entry[\"taxID\"] = \"taxon:5911\"\n",
    "\n",
    "# and then entry JN563930.1\n",
    "JN563930_1_entry = intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict['JN563930.1']\n",
    "JN563930_1_entry[\"organism\"] = \"Nicotiana undulata\"\n",
    "JN563930_1_entry[\"taxID\"] = \"taxon:118713\"\n",
    "JN563930_1_entry[\"organelle\"] = \"plastid:chloroplast\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to classify the introns into organism types based on the taxonomy ID. We use a local copy of NCBI's taxonomy database for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi = NCBITaxa()\n",
    "# Update the local database\n",
    "ncbi.update_taxonomy_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_intron(key_value):\n",
    "    ncbi = NCBITaxa()\n",
    "    key, value = key_value\n",
    "    taxid_number = value.split(\":\")[1]\n",
    "    recovery_successful = False\n",
    "    try:\n",
    "        lineage = ncbi.get_lineage(taxid_number)\n",
    "        recovery_successful = True\n",
    "    except Exception as e:\n",
    "        # then we are going to try to do the organism search from organisn name\n",
    "        # first we get the organism name from the flattened dictionary using the same key\n",
    "        organism_name = intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict[key]['organism']\n",
    "        # then if the organism name contains the substring \"aff. \", we remove it\n",
    "        if \"aff. \" in organism_name:\n",
    "            organism_name = organism_name.replace(\"aff. \", \"\")\n",
    "        # then we are going to search for the organism name in the NCBI taxonomy database\n",
    "        taxid_number = ncbi.get_name_translator([organism_name])\n",
    "        taxid_number = taxid_number[organism_name][0]\n",
    "        # and then we try with this new taxid_number, using error handling in case it is again not found\n",
    "        try:\n",
    "            lineage = ncbi.get_lineage(taxid_number)\n",
    "            recovery_successful = True\n",
    "        except Exception as e:\n",
    "            print(f\"No result found for organism name: {organism_name}. Error: {e}\")\n",
    "            category = \"other\"\n",
    "\n",
    "    if recovery_successful:\n",
    "        names = ncbi.get_taxid_translator(lineage)\n",
    "        names_values = names.values()\n",
    "        lineages_dict[key] = names_values\n",
    "    else:\n",
    "        # this should not happen for any key\n",
    "        lineages_dict[key] = None\n",
    "        print(f\"No result found for organism name: {organism_name}. Error: {e}\")\n",
    "        category = \"other\"\n",
    "    if \"Bacteria\" in names_values:\n",
    "        category = \"bacteria\"\n",
    "    elif 'Viruses' in names_values:\n",
    "        category = \"virus\"\n",
    "    elif \"Eukaryota\" in names_values:\n",
    "        if \"Viridiplantae\" in names_values or 'Diphylleia' in names_values:\n",
    "            category = \"plants\"\n",
    "        elif \"Mollusca\" in names_values:\n",
    "            category = \"molluscs\"\n",
    "        elif \"Fungi\" in names_values:\n",
    "            category = \"fungi\"\n",
    "        elif \"Ciliophora\" in names_values:\n",
    "            category = \"ciliates\"\n",
    "        elif \"Oomycota\" in names_values:\n",
    "            category = \"oomycetes\"\n",
    "        elif 'Acanthamoeba' in names_values or 'Amoebidium' in names_values or 'Dictyostelia' in names_values or 'Dictyostelium' in names_values or 'Heterostelium pallidum' in names_values or 'Myxogastria' in names_values or 'Physariida' in names_values or any(\"amoeba\" in element for element in names_values) or 'Amoebozoa' in names_values or 'Nuclearia' in names_values:\n",
    "            category = \"amoebae\"\n",
    "        elif 'Bacillariophyta' in names_values:\n",
    "            category = \"diatoms\"\n",
    "        elif 'Chlorarachniophyceae' in names_values:\n",
    "            category = \"green algae\"\n",
    "        elif 'Choanoflagellata' in names_values:\n",
    "            category = \"choanoflagellates\"\n",
    "        elif 'Cryptophyceae' in names_values:\n",
    "            category = \"cryptophytes\"\n",
    "        elif 'Cyanophora' in names_values:\n",
    "            category = \"glaucophytes\"\n",
    "        elif 'Euglenida' in names_values:\n",
    "            category = \"euglenids\"\n",
    "        elif 'Eustigmatophyceae' in names_values:\n",
    "            category = \"eustigmatophytes\"\n",
    "        elif 'Heterolobosea' in names_values:\n",
    "            category = \"percolozoa\"\n",
    "        elif 'Heteromitidae' in names_values:\n",
    "            category = \"cercomonads\"\n",
    "        elif 'Phaeophyceae' in names_values or 'Schizocladia' in names_values:\n",
    "            category = \"brown algae\"\n",
    "        elif 'Plasmodiophorida' in names_values:\n",
    "            category = \"plasmodiophores\"\n",
    "        elif 'Porifera' in names_values:\n",
    "            category = \"sponges\"\n",
    "        elif 'Rhodophyta' in names_values:\n",
    "            category = \"red algae\"\n",
    "        elif 'Xanthophyceae' in names_values:\n",
    "            category = \"yellow-green algae\"\n",
    "        elif 'Hexacorallia' in names_values:\n",
    "            category = \"corals\"\n",
    "        elif 'Insecta' in names_values:\n",
    "            category = \"insects\"\n",
    "        elif 'Vertebrata' in names_values:\n",
    "            category = \"vertebrates\"\n",
    "        elif 'Cercozoa' in names_values:\n",
    "            category = \"cercozoans\"\n",
    "        elif 'Centroplasthelida' in names_values:\n",
    "            category = \"centrohelids\"\n",
    "        elif 'Placozoa' in names_values:\n",
    "            category = \"placozoans\"\n",
    "        else:\n",
    "            category = \"other\"\n",
    "        # note that many of \n",
    "    else:\n",
    "        category = \"other\"\n",
    "    entry_to_update = intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict[key]\n",
    "    extended_entry = entry_to_update.copy()\n",
    "    extended_entry[\"organismType\"] = category\n",
    "    extended_entry[\"lineage\"] = list(names_values)\n",
    "    return key, extended_entry\n",
    "\n",
    "classified_introns = {}\n",
    "lineages_dict = {}\n",
    "# Extract all taxon IDs\n",
    "taxon_ids = {key: entry['taxID'] for key, entry in intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict.items() if entry['taxID']}\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future_to_key = {executor.submit(classify_intron, item): item[0] for item in taxon_ids.items()}\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_to_key)):\n",
    "        key = future_to_key[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (key, exc))\n",
    "        else:\n",
    "            classified_introns[result[0]] = result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save this dictionary with categories of organisms as well. It also contains the entire taxonomic lineages of each organism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classified_introns.pkl', 'wb') as f:\n",
    "    pickle.dump(classified_introns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classified_introns.pkl', 'rb') as f:\n",
    "    classified_introns = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a barplot with the number of introns in each organism group. For this plot, we group all protists other than amoebae under category \"protists\". We group corals and sponges together also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the organism types\n",
    "organism_types = [entry['organismType'] for entry in classified_introns.values()]\n",
    "\n",
    "# Count the occurrences of each organism type\n",
    "counts = Counter(organism_types)\n",
    "\n",
    "# Create a new dictionary where we group the categories for other protists\n",
    "grouped_counts = {'plants': 0, 'fungi': 0, 'amoebae': 0, 'virus': 0, 'bacteria': 0, 'corals/sponges': 0, 'other protists': 0}\n",
    "for organism_type, count in counts.items():\n",
    "    if organism_type in grouped_counts:\n",
    "        grouped_counts[organism_type] += count\n",
    "    elif organism_type in ['corals', 'sponges']:\n",
    "        grouped_counts['corals/sponges'] += count\n",
    "    else:\n",
    "        grouped_counts['other protists'] += count\n",
    "\n",
    "# Sort the dictionary by the frequencies, except 'other protists' which should be last\n",
    "sorted_counts = {k: v for k, v in sorted(grouped_counts.items(), key=lambda item: (-item[1], item[0] == 'other protists'))}\n",
    "\n",
    "# Create the barplot\n",
    "\n",
    "plt.rc('font', family='Arial', size=9)\n",
    "plt.figure(figsize=(5.2/2.54, 5.2/2.54))\n",
    "\n",
    "plt.bar(sorted_counts.keys(), sorted_counts.values())\n",
    "plt.xlabel('Organism Type')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('Figure1b.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a similar barplot but excluding plants and fungi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary excluding 'plants' and 'fungi'\n",
    "filtered_counts = {k: v for k, v in counts.items() if k not in ['plants', 'fungi']}\n",
    "\n",
    "# Create the barplot\n",
    "plt.rc('font', family='Arial', size=8)\n",
    "plt.figure(figsize=(10/2.54, 5.2/2.54))\n",
    "\n",
    "plt.bar(filtered_counts.keys(), filtered_counts.values())\n",
    "plt.xlabel('Organism Type')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('Figure1c.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to identify putative homing endonucleases in the introns. We first extract all possible ORFs of a minimum length in all reading frames, taking into consideration that translation to protein should be done with the appropriate genetic code taking into account organism and subcellular location. The correct genetic code also defines with which codons a candidate ORF can start and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio.Data import CodonTable\n",
    "\n",
    "def find_orfs(sequence, frame, genetic_code, min_length):\n",
    "    start_codons = CodonTable.unambiguous_dna_by_id[genetic_code].start_codons\n",
    "    if frame < 3:\n",
    "        sequence_frame = sequence[frame:]\n",
    "    else:\n",
    "        sequence_frame = str(Seq(sequence).reverse_complement())[frame-3:]\n",
    "    protein = str(Seq(sequence_frame).translate(table=genetic_code, to_stop=False))\n",
    "    orfs = []\n",
    "    for i in range(0, len(sequence_frame), 3):\n",
    "        if sequence_frame[i:i+3] in start_codons:\n",
    "            protein_start = i // 3\n",
    "            for j in range(protein_start, len(protein)):\n",
    "                if protein[j] == \"*\":\n",
    "                    orf = protein[protein_start:j]\n",
    "                    if len(orf) >= min_length:\n",
    "                        orfs.append({\n",
    "                            'sequence': orf,\n",
    "                            'startInIntronSeq': frame + i,\n",
    "                            'endInIntronSeq': frame + i + len(orf) * 3,\n",
    "                            'geneticCode': genetic_code,\n",
    "                            'readingFrame': frame\n",
    "                        })\n",
    "                    break\n",
    "    # Remove ORFs that are fully contained within another ORF\n",
    "    orfs = [orf1 for orf1 in orfs if not any(orf2['startInIntronSeq'] <= orf1['startInIntronSeq'] and orf2['endInIntronSeq'] >= orf1['endInIntronSeq'] for orf2 in orfs if orf2 != orf1)]\n",
    "    return orfs\n",
    "\n",
    "min_length = 120\n",
    "orf_dict = {}\n",
    "for key, value in classified_introns.items():\n",
    "    orf_dict[key] = {}\n",
    "    lineage = [tax.lower() for tax in value['lineage']]\n",
    "    category = value['organismType']\n",
    "    organelle = value['organelle']\n",
    "    if (\"mitochondria\" in organelle):\n",
    "        if ((\"saccharomyces\" in lineage and \"cerevisiae\" in lineage) or (\"candida\" in lineage and \"glabrata\" in lineage) or \n",
    "            (\"hansenula\" in lineage and \"saturnus\" in lineage) or (\"kluyveromyces\" in lineage and \"thermotolerans\" in lineage)):\n",
    "            genetic_code = 3\n",
    "        elif ((\"emericella\" in lineage and \"nidulans\" in lineage) or (\"neurospora\" in lineage and \"crassa\" in lineage) or\n",
    "              (\"podospora\" in lineage and \"anserina\" in lineage) or (\"acremonium\" in lineage) or (\"candida\" in lineage and \"parapsilosis\" in lineage) or\n",
    "              (\"trichophyton\" in lineage and \"rubrum\" in lineage) or (\"dekkera\" in lineage) or (\"brettanomyces\" in lineage) or \n",
    "              (\"eeniella\" in lineage) or (\"ascobolus\" in lineage and \"immersus\" in lineage) or (\"aspergillus\" in lineage and \"amstelodami\" in lineage) or\n",
    "              (\"claviceps\" in lineage and \"purpurea\" in lineage) or (\"cochliobolus\" in lineage and \"heterostrophus\" in lineage) or \n",
    "              (\"gigartinales\" in lineage) or (\"trypanosoma\" in lineage and \"bruceii\" in lineage) or (\"leishmania\" in lineage and \"tarentolae\" in lineage) or\n",
    "              (\"paramecium\" in lineage and \"tetraurelia\" in lineage) or (\"tetrahymena\" in lineage and \"pyriformis\" in lineage) or\n",
    "              (\"plasmodium\" in lineage and \"gallinaceum\" in lineage) or (\"coelenterata\" in lineage)):\n",
    "            genetic_code = 4\n",
    "        elif ((\"asterozoa\" in lineage) or (\"echinozoa\" in lineage) or (\"rhabditophora\" in lineage)):\n",
    "            genetic_code = 9\n",
    "        elif ((\"pyura\" in lineage and \"stolonifera\" in lineage) or (\"halocynthia\" in lineage and \"roretzi\" in lineage) or \n",
    "              (\"ciona\" in lineage and \"savignyi\" in lineage)):\n",
    "              #(\"ciona\" in lineage and \"savignyi\" in lineage) or (\"halocynthia\" in lineage and \"roretzi\" in lineage)):\n",
    "            genetic_code = 13\n",
    "        elif ((\"chlorophyceae\" in lineage) or (\"spizellomyces\" in lineage and \"punctatus\" in lineage)):\n",
    "            genetic_code = 16\n",
    "        elif \"trematoda\" in lineage:\n",
    "            genetic_code = 21\n",
    "        elif ((\"scenedesmus\" in lineage and \"obliquus\" in lineage)):\n",
    "            genetic_code = 22\n",
    "        elif ((\"thraustochytrium\" in lineage and \"aureum\" in lineage)):\n",
    "            genetic_code = 23\n",
    "        elif ((\"rhabdopleuridae\" in lineage and \"compacta\" in lineage)):\n",
    "            genetic_code = 24\n",
    "        elif ((\"vertebrata\" in lineage)):\n",
    "            genetic_code = 2\n",
    "        elif ((\"ascaris\" in lineage) or (\"caenorhabditis\" in lineage) or (\"bivalvia\" in lineage) or (\"polyplacophora\" in lineage) or\n",
    "              (\"artemia\" in lineage) or (\"drosophila\" in lineage) or (\"locusta\" in lineage and \"migratoria\" in lineage) or\n",
    "              (\"apis\" in lineage and \"mellifera\" in lineage)):\n",
    "            genetic_code = 5\n",
    "        elif ((\"platyhelminthes\" in lineage) or (\"nematoda\" in lineage)):\n",
    "            genetic_code = 14\n",
    "        else:\n",
    "            genetic_code = 1\n",
    "    elif (organelle == \"\"):\n",
    "        if ((\"oxytricha\" in lineage) or (\"stylonychia\" in lineage) or (\"paramecium\" in lineage) or (\"tetrahymena\" in lineage) or\n",
    "            (\"oxytrichidae\" in lineage) or (\"glaucoma\" in lineage and \"chattoni\" in lineage)):\n",
    "            genetic_code = 6\n",
    "        elif ((\"euplotidae\" in lineage)):\n",
    "            genetic_code = 10\n",
    "        elif category == \"bacteria\":\n",
    "            genetic_code = 11\n",
    "        elif ((\"cephaloascaceae\" in lineage) or (\"debaryomycetaceae\" in lineage) or (\"metschnikowiaceae\" in lineage) or (\"babjeviella\" in lineage) or\n",
    "              (\"ascoideaceae\" in lineage) or (\"saccharomycopsidaceae\" in lineage)):\n",
    "            genetic_code = 12\n",
    "        elif ((\"blepharisma\" in lineage) or (\"crassvirales\" in lineage)):\n",
    "            genetic_code = 15\n",
    "        elif ((\"candidate division sr1\" in lineage) or (\"gracilibacteria\" in lineage)):\n",
    "            genetic_code = 25\n",
    "        elif ((\"pachysolen\" in lineage) or (\"nakazawaea\" in lineage) or (\"peterozyma\" in lineage)):\n",
    "            genetic_code = 26\n",
    "        elif ((\"parduczia\" in lineage)):\n",
    "            genetic_code = 27\n",
    "        elif ((\"condylostoma\" in lineage and \"magnum\" in lineage)):\n",
    "            genetic_code = 28\n",
    "        elif ((\"mesodinium\" in lineage) or (\"myrionecta\" in lineage)):\n",
    "            genetic_code = 29\n",
    "        elif ((\"carchesium\" in lineage)):\n",
    "            genetic_code = 30\n",
    "        elif ((\"blastocrithidia\" in lineage)):\n",
    "            genetic_code = 31\n",
    "        elif ((\"cephalodiscidae\" in lineage)):\n",
    "            genetic_code = 33\n",
    "        else:\n",
    "            genetic_code = 1\n",
    "    elif (\"plastid\" in organelle):\n",
    "        if category == \"plants\":\n",
    "            genetic_code = 11\n",
    "        else:\n",
    "            genetic_code = 1\n",
    "    for frame in range(6):\n",
    "        orf_dict[key][frame] = find_orfs(value['sequence'], frame, genetic_code, min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_orfs = sum(len(orfs) for frame_orfs in orf_dict.values() for orfs in frame_orfs.values())\n",
    "print(f\"Total number of candidate ORFs: {total_orfs}\")\n",
    "\n",
    "introns_with_orfs = len([key for key, value in orf_dict.items() if any(value.values())])\n",
    "print(f\"Number of introns with candidate ORFs: {introns_with_orfs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"orfs.fasta\", \"w\") as output_handle:\n",
    "    for key, value in orf_dict.items():\n",
    "        for frame, orfs in value.items():\n",
    "            for i, orf in enumerate(orfs):\n",
    "                sequence = Seq(orf['sequence'])\n",
    "                record = SeqRecord(sequence, id=f\"{key}_frame{frame}_orf{i}\", description=\"\")\n",
    "                SeqIO.write(record, output_handle, \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a run_interproscan.sh file that will be executed to run InterProScan on all ORFs. The file should be something like this:\n",
    "    \n",
    "```bash\n",
    "    #!/bin/bash\n",
    "    /path/to/interproscan.sh -i orfs.fasta -f json -o orfs.json -cpu 128\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the shell script\n",
    "command = [\"bash\", \"run_interproscan.sh\"]\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "if process.returncode != 0:\n",
    "    print(f\"InterProScan failed with error message:\\n{stderr.decode()}\")\n",
    "else:\n",
    "    print(\"InterProScan completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the InterProScan results and add them to a dictionary\n",
    "interpro_results = {}\n",
    "with open('orfs.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for protein in data['results']:\n",
    "        id_parts = protein['xref'][0]['id'].split('_')\n",
    "        key = '_'.join(id_parts[:-2])\n",
    "        frame = int(id_parts[-2][-1])\n",
    "        orf_index = int(id_parts[-1][3:])\n",
    "        if key not in interpro_results:\n",
    "            interpro_results[key] = {}\n",
    "        if frame not in interpro_results[key]:\n",
    "            interpro_results[key][frame] = {}\n",
    "        interpro_results[key][frame][orf_index] = protein['matches']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make a new dictionary where we only keep introns for which an ORF likely to be a homing endonuclease was found. We also add the InterProScan hits for these cases. It is possible that some introns might contain multiple homing endonucleases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_substrings(d):\n",
    "    # Check if the current dictionary contains the substrings\n",
    "    if any(\"endonuc\" in value.lower() or \"homing\" in value.lower() or \"nuclease\" in value.lower() \n",
    "           for value in d.values() if isinstance(value, str)):\n",
    "        return True\n",
    "    # Recursively check the nested dictionaries\n",
    "    return any(check_substrings(value) for value in d.values() if isinstance(value, dict))\n",
    "\n",
    "# Make a deep copy of the orf_dict dictionary\n",
    "homingEndonucleases_dict = copy.deepcopy(orf_dict)\n",
    "discarded_ORFs_dict = copy.deepcopy(orf_dict)\n",
    "\n",
    "# Iterate over each intron in the copied dictionary\n",
    "for intron, frames in list(homingEndonucleases_dict.items()):  # Use list to allow modifying the dictionary during iteration\n",
    "    # For each intron, iterate over each frame\n",
    "    for frame, orfs in list(frames.items()):  # Use list to allow modifying the dictionary during iteration\n",
    "        # Create a new list that only includes the ORFs you want to keep\n",
    "        new_orfs = []\n",
    "        # For each frame, iterate over each ORF\n",
    "        for i, orf in enumerate(orfs):\n",
    "            # Check if the ORF has any InterProScan hits\n",
    "            if intron in interpro_results and frame in interpro_results[intron] and i in interpro_results[intron][frame]:\n",
    "                hits = interpro_results[intron][frame][i]\n",
    "                # Check if any hit contains the substrings \"endonuc\", \"homing\", or \"nuclease\", ignoring case\n",
    "                if any(check_substrings(hit) for hit in hits):\n",
    "                    # Append the list of InterProScan hits to the ORF's dictionary\n",
    "                    orf['interProScanHits'] = hits\n",
    "                    # Add the ORF to the new list\n",
    "                    new_orfs.append(orf)\n",
    "                else:\n",
    "                    # Append the list of InterProScan hits to the ORF's dictionary in discarded_ORFs_dict\n",
    "                    discarded_ORFs_dict[intron][frame][i]['interProScanHits'] = hits\n",
    "            else:\n",
    "                # Treat the ORF as if there were hits but they did not contain any of the specified substrings\n",
    "                orf['interProScanHits'] = []\n",
    "        # Replace the old list of ORFs with the new list\n",
    "        frames[frame] = new_orfs\n",
    "    # Remove the intron if all its frames are empty\n",
    "    if all(not orfs for orfs in frames.values()):\n",
    "        del homingEndonucleases_dict[intron]\n",
    "\n",
    "# Iterate over each intron in the discarded_ORFs_dict dictionary\n",
    "for intron, frames in list(discarded_ORFs_dict.items()):  # Use list to allow modifying the dictionary during iteration\n",
    "    # For each intron, iterate over each frame\n",
    "    for frame, orfs in list(frames.items()):  # Use list to allow modifying the dictionary during iteration\n",
    "        # Remove the frame if all its ORFs have no InterProScan hits\n",
    "        if all('interProScanHits' not in orf or not orf['interProScanHits'] for orf in orfs):\n",
    "            del frames[frame]\n",
    "    # Remove the intron if all its frames are empty\n",
    "    if all(not orfs for orfs in frames.values()):\n",
    "        del discarded_ORFs_dict[intron]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homingEndonucleases_dict_reshaped = {}\n",
    "\n",
    "# Iterate over each intron in homingEndonucleases_dict\n",
    "for intron, frames in homingEndonucleases_dict.items():\n",
    "    # Initialize an empty list for the current intron\n",
    "    homingEndonucleases_dict_reshaped[intron] = []\n",
    "    # For each intron, iterate over each frame\n",
    "    for frame, orfs in frames.items():\n",
    "        # For each frame, iterate over each ORF\n",
    "        for orf in orfs:\n",
    "            # Create a new dictionary that is a copy of the ORF's dictionary\n",
    "            orf_copy = copy.deepcopy(orf)\n",
    "            # Add a new key to the new dictionary with the name 'frame' and the value of the current frame\n",
    "            orf_copy['frame'] = frame\n",
    "            # Append the new dictionary to the list of ORFs for the current intron\n",
    "            homingEndonucleases_dict_reshaped[intron].append(orf_copy)\n",
    "\n",
    "print(homingEndonucleases_dict_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save this dictionary for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('homingEndonucleases_dict_reshaped.pkl', 'wb') as f:\n",
    "    pickle.dump(homingEndonucleases_dict_reshaped, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('homingEndonucleases_dict_reshaped.pkl', 'rb') as f:\n",
    "    homingEndonucleases_dict_reshaped = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets count the total number of homing endonucleases identified. In order to do so, we sum the length of each list stored as the value of each key of homingEndonucleases_dict_reshaped\n",
    "\n",
    "total_homing_endonucleases = sum(len(value) for value in homingEndonucleases_dict_reshaped.values())\n",
    "\n",
    "print(f\"Total number of putative homing endonucleases: {total_homing_endonucleases}\")\n",
    "\n",
    "print(f\"These are located in a total of {len(homingEndonucleases_dict_reshaped)} introns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a dictionary where each key is a number of endonucleases per intron and each value is a list with the introns that have that number of endonucleases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endonuclease_counter = Counter(len(value) for value in homingEndonucleases_dict_reshaped.values())\n",
    "\n",
    "print(endonuclease_counter)\n",
    "\n",
    "introns_by_number_endonucleases = {f\"{count} endonuclease\" if count == 1 else f\"{count} endonucleases\": [] for count in endonuclease_counter.keys()}\n",
    "for key, value in homingEndonucleases_dict_reshaped.items():\n",
    "    count = len(value)\n",
    "    introns_by_number_endonucleases[f\"{count} endonuclease\" if count == 1 else f\"{count} endonucleases\"].append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the key for the intron with 4 detected endonucleases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(introns_by_number_endonucleases[\"4 endonucleases\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to compare the histogram of lengths for all introns and for introns where homing endonucleases were detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the sizes of the introns from the classified_introns dictionary\n",
    "all_intron_sizes = [len(intron[\"sequence\"]) for intron in classified_introns.values()]\n",
    "\n",
    "# Extract the sizes of the introns from the homingEndonucleases_dict_reshaped dictionary\n",
    "homing_endonuclease_intron_sizes = [len(classified_introns[intron][\"sequence\"]) for intron in homingEndonucleases_dict_reshaped if intron in classified_introns]\n",
    "\n",
    "min_size = min(all_intron_sizes)\n",
    "max_size = max(all_intron_sizes)\n",
    "\n",
    "# Create a histogram for the distribution of intron sizes of all introns\n",
    "plt.rc('font', family='Arial', size=8)\n",
    "plt.figure(figsize=(15/2.54, 7.5/2.54))\n",
    "\n",
    "plt.hist(all_intron_sizes, bins=60, alpha=0.5, color=\"blue\", range=(min_size, max_size), label='All introns', density=True)\n",
    "\n",
    "# Create a histogram for the distribution of intron sizes of homing endonuclease introns\n",
    "plt.hist(homing_endonuclease_intron_sizes, bins=60, alpha=0.5, color=\"red\", range=(min_size, max_size), label='Introns with homing endonucleases', density=True)\n",
    "\n",
    "plt.xlabel('Intron size')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.savefig('Figure1d.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to perform secondary structure predictions. First let's set up the arnie.conf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"eternafold: /path/to/EternaFold/src\" > arnie.conf\n",
    "!echo \"vienna_2: /path/to/ViennaRNA/bin\" >> arnie.conf\n",
    "!echo \"nupack: /path/to/nupack3/bin\" >> arnie.conf\n",
    "!echo \"contrafold_2: /path/to/contrafold-se/src\" >> arnie.conf\n",
    "!echo \"rnastructure: /path/to/RNAstructure/exe\" >> arnie.conf\n",
    "!echo \"rnasoft: /path/to/MultiRNAFold\" >> arnie.conf\n",
    "!echo \"linearfold: /path/to/LinearFold/bin\" >> arnie.conf\n",
    "!echo \"linearpartition: /path/to/LinearPartition/bin\" >> arnie.conf\n",
    "!echo \"spotrna: /path/to/SPOT-RNA\" >> arnie.conf\n",
    "!echo \"ipknot: /path/to/ipknot/bin\" >> arnie.conf\n",
    "!echo \"TMP: /path/to/tmp/folder/\" >> arnie.conf\n",
    "\n",
    "os.environ[\"ARNIEFILE\"] = f'/path/to/arnie.conf'\n",
    "os.environ[\"DATAPATH\"] = f'/path/to/RNAstructure/data_tables'\n",
    "\n",
    "import arnie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we run predictions of secondary structure and base-pairing probabilities for all introns with all available software. We start with mean free energy (MFE) secondary structure predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arnie.mfe import mfe\n",
    "\n",
    "symbol_mapping = {\n",
    "    'N': 'A',\n",
    "    'S': 'C',\n",
    "    'Y': 'C',\n",
    "    'B': 'C',\n",
    "    'R': 'A',\n",
    "    'D': 'A',\n",
    "    'W': 'A',\n",
    "    'K': 'G',\n",
    "    'V': 'A',\n",
    "    'H': 'A',\n",
    "    'M': 'A',\n",
    "    'T': 'U'\n",
    "}\n",
    "\n",
    "def process_intron(key, value):\n",
    "    sequence = value['sequence']\n",
    "    for non_standard, standard in symbol_mapping.items():\n",
    "        sequence = sequence.replace(non_standard, standard)\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        os.chdir(tmpdir)\n",
    "        try:\n",
    "            eternafold_MFE_structure = mfe(sequence, package='eternafold')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with eternafold: {e}\")\n",
    "            eternafold_MFE_structure = e\n",
    "        try:\n",
    "            vienna_MFE_structure = mfe(sequence, package='vienna_2', DEBUG=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with vienna_2: {e}\")\n",
    "            vienna_MFE_structure = e\n",
    "        try:\n",
    "            contrafold_MFE_structure = mfe(sequence, package='contrafold_2', viterbi=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with contrafold_2: {e}\")\n",
    "            contrafold_MFE_structure = e\n",
    "        try:\n",
    "            rnastructure_MFE_structure = mfe(sequence, package='rnastructure', pseudo=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with rnastructure: {e}\")\n",
    "            rnastructure_MFE_structure = e\n",
    "    return key, eternafold_MFE_structure, vienna_MFE_structure, contrafold_MFE_structure, rnastructure_MFE_structure\n",
    "\n",
    "eternafold_MFE_secondary_structures = {}\n",
    "vienna_MFE_secondary_structures = {}\n",
    "contrafold_MFE_secondary_structures = {}\n",
    "rnastructure_MFE_secondary_structures = {}\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_intron, key, value): key for key, value in classified_introns.items()}\n",
    "    pbar = tqdm(total=len(futures), desc=\"Processing introns\", dynamic_ncols=True)\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        key, eternafold_MFE_structure, vienna_MFE_structure, contrafold_MFE_structure, rnastructure_MFE_structure = future.result()\n",
    "        eternafold_MFE_secondary_structures[key] = eternafold_MFE_structure\n",
    "        vienna_MFE_secondary_structures[key] = vienna_MFE_structure\n",
    "        contrafold_MFE_secondary_structures[key] = contrafold_MFE_structure\n",
    "        rnastructure_MFE_secondary_structures[key] = rnastructure_MFE_structure\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "with open('results_secondary_structures_MFE.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'eternafold': eternafold_MFE_secondary_structures,\n",
    "        'vienna': vienna_MFE_secondary_structures,\n",
    "        'contrafold': contrafold_MFE_secondary_structures,\n",
    "        'rnastructure': rnastructure_MFE_secondary_structures\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can read back the MFE secondary structures\n",
    "\n",
    "with open('results_secondary_structures_MFE.pkl', 'rb') as f:\n",
    "    MFE_secondary_structures = pickle.load(f)\n",
    "\n",
    "eternafold_MFE_secondary_structures = MFE_secondary_structures['eternafold']\n",
    "vienna_MFE_secondary_structures = MFE_secondary_structures['vienna']\n",
    "contrafold_MFE_secondary_structures = MFE_secondary_structures['contrafold']\n",
    "rnastructure_MFE_secondary_structures = MFE_secondary_structures['rnastructure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to calculate base pair probability (BPP) matrixes for all introns with each package. Note this will take a long time, and require large amounts of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arnie.bpps import bpps\n",
    "\n",
    "symbol_mapping = {\n",
    "    'N': 'A',\n",
    "    'S': 'C',\n",
    "    'Y': 'C',\n",
    "    'B': 'C',\n",
    "    'R': 'A',\n",
    "    'D': 'A',\n",
    "    'W': 'A',\n",
    "    'K': 'G',\n",
    "    'V': 'A',\n",
    "    'H': 'A',\n",
    "    'M': 'A',\n",
    "    'T': 'U'\n",
    "}\n",
    "\n",
    "def process_intron_bpps(key, value):\n",
    "    sequence = str(value['sequence'])\n",
    "    for non_standard, standard in symbol_mapping.items():\n",
    "        sequence = sequence.replace(non_standard, standard)\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        os.chdir(tmpdir)\n",
    "        try:\n",
    "            eternafold_BPPS = bpps(sequence, package='eternafold')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with eternafold: {e}\")\n",
    "            eternafold_BPPS = e\n",
    "        try:\n",
    "            vienna_BPPS = bpps(sequence, package='vienna_2')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with vienna_2: {e}\")\n",
    "            vienna_BPPS = e\n",
    "        try:\n",
    "            contrafold_BPPS = bpps(sequence, package='contrafold_2')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with contrafold_2: {e}\")\n",
    "            contrafold_BPPS = e\n",
    "        try:\n",
    "            rnastructure_BPPS = bpps(sequence, package='rnastructure')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with rnastructure: {e}\")\n",
    "            rnastructure_BPPS = e\n",
    "        try:\n",
    "            rnasoft_BPPS = bpps(sequence, package='rnasoft')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with rnasoft: {e}\")\n",
    "            rnasoft_BPPS = e\n",
    "        try:\n",
    "            nupack_BPPS = bpps(sequence, package='nupack')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with nupack: {e}\")\n",
    "            nupack_BPPS = e\n",
    "    return key, eternafold_BPPS, vienna_BPPS, contrafold_BPPS, rnastructure_BPPS, rnasoft_BPPS, nupack_BPPS\n",
    "\n",
    "eternalfold_BPPS_matrixes = {}\n",
    "vienna_BPPS_matrixes = {}\n",
    "contrafold_BPPS_matrixes = {}\n",
    "rnastructure_BPPS_matrixes = {}\n",
    "rnasoft_BPPS_matrixes = {}\n",
    "nupack_BPPS_matrixes = {}\n",
    "\n",
    "eternalfold_BPPS_matrixes_errors = {}\n",
    "vienna_BPPS_matrixes_errors = {}\n",
    "contrafold_BPPS_matrixes_errors = {}\n",
    "rnastructure_BPPS_matrixes_errors = {}\n",
    "rnasoft_BPPS_matrixes_errors = {}\n",
    "nupack_BPPS_matrixes_errors = {}\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_intron_bpps, key, value): key for key, value in classified_introns.items()}\n",
    "    pbar = tqdm(total=len(futures), desc=\"Processing introns\", dynamic_ncols=True)\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        key, eternafold_BPPS, vienna_BPPS, contrafold_BPPS, rnastructure_BPPS, rnasoft_BPPS, nupack_BPPS = future.result()\n",
    "        if isinstance(eternafold_BPPS, Exception):\n",
    "            eternalfold_BPPS_matrixes_errors[key] = eternafold_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/eternafold/{key}_eternafold.npy\", \"wb\") as f:\n",
    "                np.save(f, eternafold_BPPS)\n",
    "        if isinstance(vienna_BPPS, Exception):\n",
    "            vienna_BPPS_matrixes_errors[key] = vienna_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/vienna/{key}_vienna.npy\", \"wb\") as f:\n",
    "                np.save(f, vienna_BPPS)\n",
    "        if isinstance(contrafold_BPPS, Exception):\n",
    "            contrafold_BPPS_matrixes_errors[key] = contrafold_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/contrafold/{key}_contrafold.npy\", \"wb\") as f:\n",
    "                np.save(f, contrafold_BPPS)\n",
    "        if isinstance(rnastructure_BPPS, Exception):\n",
    "            rnastructure_BPPS_matrixes_errors[key] = rnastructure_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/rnastructure/{key}_rnastructure.npy\", \"wb\") as f:\n",
    "                np.save(f, rnastructure_BPPS)\n",
    "        if isinstance(rnasoft_BPPS, Exception):\n",
    "            rnasoft_BPPS_matrixes_errors[key] = rnasoft_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/rnasoft/{key}_rnasoft.npy\", \"wb\") as f:\n",
    "                np.save(f, rnasoft_BPPS)\n",
    "        if isinstance(nupack_BPPS, Exception):\n",
    "            nupack_BPPS_matrixes_errors[key] = nupack_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/nupack/{key}_nupack.npy\", \"wb\") as f:\n",
    "                np.save(f, nupack_BPPS)\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPPs run successfully with all software for all introns except for a set of 106 introns, which fail for RNAsoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_keys_rnastructure_BPPS_matrixes = list(rnastructure_BPPS_matrixes_errors.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the preceding and following exonic context. Extracted intron sequences already contained the last base of the preceding exon and the first base of the following exon; these are included again in the exonic contexts. Up to 30 bases of each are extracted (if there are not enough bases available, extracted sequences are padded with \"-\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_introns_preceding_exons = {}\n",
    "classified_introns_following_exons = {}\n",
    "number_bases_preceding_exon = 30\n",
    "number_bases_following_exon = 30\n",
    "\n",
    "for key, value in tqdm(classified_introns.items(), total=len(classified_introns)):\n",
    "    start_position_intron = value['startPosition']\n",
    "    end_position_intron = value['endPosition'] + 1\n",
    "    genbankID = value['GenBankID']\n",
    "    genbank_record = infernal_NT_search_genbank_recs[genbankID]\n",
    "    if genbank_record.seq.defined:\n",
    "        genbank_sequence_raw = genbank_record.seq\n",
    "    else:\n",
    "        genbank_sequence_raw = seqs_of_records_with_undefined_seqs[genbankID]\n",
    "    genbank_sequence = genbank_sequence_raw.replace('U', 'T')\n",
    "    source = value['boundariesSource']\n",
    "    if(\"reverseStrand\" in source):\n",
    "        reverseStrand = True\n",
    "    else:\n",
    "        reverseStrand = False\n",
    "    if(reverseStrand):\n",
    "        following_exon_seq = genbank_sequence_raw[max((start_position_intron - number_bases_following_exon + 1), 0):(start_position_intron+1)].reverse_complement()\n",
    "        preceding_exon_seq = genbank_sequence_raw[(end_position_intron-1):(end_position_intron + number_bases_preceding_exon - 1)].reverse_complement()\n",
    "    else:\n",
    "        following_exon_seq = genbank_sequence_raw[(end_position_intron-1):(end_position_intron + number_bases_following_exon - 1)]\n",
    "        preceding_exon_seq = genbank_sequence_raw[max((start_position_intron - number_bases_preceding_exon + 1), 0):(start_position_intron+1)]\n",
    "    if(len(following_exon_seq) < number_bases_following_exon):\n",
    "        following_exon_seq = following_exon_seq + \"-\"*(number_bases_following_exon - len(following_exon_seq))\n",
    "    if(len(preceding_exon_seq) < number_bases_preceding_exon):\n",
    "        preceding_exon_seq = \"-\"*(number_bases_preceding_exon - len(preceding_exon_seq)) + preceding_exon_seq\n",
    "    classified_introns_preceding_exons[key] = preceding_exon_seq\n",
    "    classified_introns_following_exons[key] = following_exon_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open(\"classified_introns_preceding_exons.pkl\", \"wb\")) as f:\n",
    "    pickle.dump(classified_introns_preceding_exons, f)\n",
    "\n",
    "with(open(\"classified_introns_following_exons.pkl\", \"wb\")) as f:\n",
    "    pickle.dump(classified_introns_following_exons, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create the database files. First the main dataframe of all introns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for key, value in classified_introns.items():\n",
    "    genbankID = value['GenBankID']\n",
    "    startPosition = int(value['startPosition']) + 1\n",
    "    endPosition = int(value['endPosition']) + 1\n",
    "    intronSequence = str(value['sequence'])\n",
    "    intronLength = value['length']\n",
    "    source = value['boundariesSource']\n",
    "    if \"reverseStrand\" in source:\n",
    "        strand = \"reverse\"\n",
    "    else:\n",
    "        strand = \"forward\"\n",
    "    organism = value['organism']\n",
    "    organismType = value['organismType']\n",
    "    subcellularLocation = value['organelle']\n",
    "    if subcellularLocation == '':\n",
    "        if organismType == \"bacteria\":\n",
    "            subcellularLocation = 'cytoplasm'\n",
    "        elif organismType == \"virus\":\n",
    "            subcellularLocation = 'virion'\n",
    "        else:\n",
    "            subcellularLocation = 'nucleus'\n",
    "    taxID = value['taxID']\n",
    "    taxID = taxID.split(\":\")[1]\n",
    "    precedingExonSequence = str(classified_introns_preceding_exons[key])\n",
    "    followingExonSequence = str(classified_introns_following_exons[key])\n",
    "    eternafoldSecondaryStructure = eternafold_MFE_secondary_structures[key]\n",
    "    viennaSecondaryStructure = vienna_MFE_secondary_structures[key]\n",
    "    contrafoldSecondaryStructure = contrafold_MFE_secondary_structures[key]\n",
    "    rnastructureSecondaryStructure = rnastructure_MFE_secondary_structures[key]\n",
    "    data.append([key, genbankID, startPosition, endPosition, intronSequence, intronLength, strand, organism, organismType, subcellularLocation, taxID, precedingExonSequence, followingExonSequence, eternafoldSecondaryStructure, viennaSecondaryStructure, contrafoldSecondaryStructure, rnastructureSecondaryStructure])\n",
    "\n",
    "intronDatabaseDF = pd.DataFrame(data, columns=['intronID', 'GenBankID', 'startPosition', 'endPosition', 'intronSequence', 'intronLength', 'strand', 'organism', 'organismType', 'subcellularLocation', 'organismTaxID', 'precedingExonSequence', 'followingExonSequence', 'eternafoldSecondaryStructure', 'viennaSecondaryStructure', 'contrafoldSecondaryStructure', 'rnastructureSecondaryStructure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intronDatabaseDF.to_csv(\"intronDatabase.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the putative endonucleases dataframe. Note that the column intronID is shared between both database files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for key, value in homingEndonucleases_dict_reshaped.items():\n",
    "    for orf in value:\n",
    "        intronID = key\n",
    "        genbankID = classified_introns[key]['GenBankID']\n",
    "        startPositionInIntron = orf['startInIntronSeq'] + 1\n",
    "        endPositionInIntron = orf['endInIntronSeq'] + 3\n",
    "        sequence = orf['sequence']\n",
    "        geneticCode = orf['geneticCode']\n",
    "        readingFrame = orf['readingFrame']\n",
    "        data.append([intronID, genbankID, startPositionInIntron, endPositionInIntron, sequence, geneticCode, readingFrame])\n",
    "    \n",
    "homingEndonucleasesDF = pd.DataFrame(data, columns=['intronID', 'GenBankID', 'startPositionInIntron', 'endPositionInIntron', 'putativeSequence', 'geneticCode', 'readingFrame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "homingEndonucleasesDF.to_csv(\"homingEndonucleasesDF.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
