{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "import os\n",
    "import certifi\n",
    "import pickle\n",
    "from Bio import Entrez, SeqIO, AlignIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.SeqFeature import CompoundLocation, ExactPosition, BeforePosition, AfterPosition\n",
    "from io import StringIO\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import string\n",
    "from ete3 import NCBITaxa\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from Bio.Data import CodonTable\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import sys\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from Bio.Blast.Applications import NcbideltablastCommandline\n",
    "from Bio.Blast import NCBIXML\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we run Infernal's cmsearch of the group I intron covariance model (obtained from Rfam) against the entire NT database. We use the parameters given in Rfam for cmsearch, but changing the database size in Mbp to the size of NT (1.3 trillion bases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables\n",
    "num_cpus = 100\n",
    "output_file_path = \"/path/to/output/tbl/out_file.tbl\"\n",
    "cm_model_path = \"/path/to/cm/model.cm\"\n",
    "input_fasta_file_path = \"/path/to/nt/database/nt\"\n",
    "path_to_cmsearch = \"/path/to/cmsearch/bin/cmsearch\"\n",
    "\n",
    "# Define the command\n",
    "command = f\"{path_to_cmsearch} --cpu {num_cpus} --verbose --nohmmonly -E 1000 -Z 1300000 --tblout {output_file_path} {cm_model_path} {input_fasta_file_path}\"\n",
    "\n",
    "# Run the command\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then read the output of cmsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_infernal_output(file_path):\n",
    "    \"\"\"Read an Infernal output file into a pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            fields = line.split(maxsplit=17)\n",
    "            data.append(fields)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = ['target name', 'accession', 'query name', 'accession', 'mdl', 'mdl from', 'mdl to', 'seq from', 'seq to', 'strand', 'trunc', 'pass', 'gc', 'bias', 'score', 'E-value', 'inc', 'description of target']\n",
    "    return df\n",
    "\n",
    "infernal_NT_search_path = output_file_path  # Path to the Infernal output file\n",
    "\n",
    "infernal_NT_search_all_hits = read_infernal_output(infernal_NT_search_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to fetch the GenBank entries of all records that gave a cmsearch hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "\n",
    "email = \"your.email@email.com\"  # Your email\n",
    "\n",
    "def fetch_genbank_records_batch(email, batch_ids, max_tries=100):\n",
    "    \"\"\"Fetch GenBank records for a batch of IDs.\"\"\"\n",
    "    Entrez.email = email\n",
    "    records = {}\n",
    "    for _ in range(max_tries):\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"nucleotide\", id=batch_ids, rettype=\"gb\", retmode=\"text\")\n",
    "            for record in SeqIO.parse(handle, \"genbank\"):\n",
    "                records[record.id] = record\n",
    "            handle.close()\n",
    "            return records\n",
    "        except:\n",
    "            time.sleep(1)  # Wait for a second before retrying\n",
    "\n",
    "def fetch_genbank_records(email, ids, batch_size=500, max_workers=10):\n",
    "    \"\"\"Fetch GenBank records for a list of IDs in batches using multiple threads.\"\"\"\n",
    "    records = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(fetch_genbank_records_batch, email, ids[i:i+batch_size]) for i in range(0, len(ids), batch_size)}\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures), 1):\n",
    "            records.update(future.result())  # Update the records dictionary with the result of the future\n",
    "            if i % 50 == 0:  # Every 50 batches\n",
    "                with open('genbank_records.pkl', 'wb') as f:\n",
    "                    pickle.dump(records, f)  # Save the records to a file, to be able to restore partial results\n",
    "                print(f\"Saved results after processing {i} batches\")\n",
    "    return records\n",
    "\n",
    "ids = list(set(infernal_NT_search_all_hits[\"target name\"].tolist()))  # Remove duplicates before passing to function\n",
    "\n",
    "infernal_NT_search_genbank_recs = fetch_genbank_records(email, ids, batch_size=50, max_workers=10)\n",
    "\n",
    "# save the final results \n",
    "with open('genbank_records.pkl', 'wb') as f:\n",
    "    pickle.dump(infernal_NT_search_genbank_recs, f) # note this will require large amount of memory to write and also to read later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('genbank_records.pkl', 'rb') as f:\n",
    "    infernal_NT_search_genbank_recs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some keys will have records with undefined sequences because their sequences are too large and must be explicitly requested as fasta. So we find these and handle them separately. We save these separately as they are very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genbank_recs_with_undefined_seqs = [key for key, value in infernal_NT_search_genbank_recs.items() if value.seq.defined is False]\n",
    "\n",
    "batch_size = 100\n",
    "genbank_ids_to_refetch = genbank_recs_with_undefined_seqs\n",
    "seqs_of_records_with_undefined_seqs = {}\n",
    "\n",
    "for i in range(0, len(genbank_ids_to_refetch), batch_size):\n",
    "    print(f\"Processing batch {i+1}-{i+batch_size}\")\n",
    "    batch_ids = genbank_ids_to_refetch[i:i+batch_size]\n",
    "    handle = Entrez.efetch(db=\"nucleotide\", id=batch_ids, rettype=\"fasta\", retmode=\"text\")\n",
    "    records = list(SeqIO.parse(handle, \"fasta\"))  # Convert iterator to list\n",
    "    with open(f'records_with_previously_undefined_seqs_batch_{i+1}_{i+batch_size}.fasta', 'w') as f:\n",
    "        SeqIO.write(records, f, \"fasta\")  # Write records to file\n",
    "    for record in records:\n",
    "        seqs_of_records_with_undefined_seqs[record.id] = record.seq\n",
    "    print(f\"Finished processing batch {i+1}-{i+batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If required we can restore the full sequences from the fasta files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_of_records_with_undefined_seqs = {}\n",
    "merged_records = []\n",
    "\n",
    "# Find all files that match the pattern\n",
    "for file_name in glob.glob(\"records_with_previously_undefined_seqs_batch_*.fasta\"):\n",
    "    # Open the file and parse the records\n",
    "    with open(file_name, \"r\") as handle:\n",
    "        records = list(SeqIO.parse(handle, \"fasta\"))\n",
    "    # Add the records to the merged_records list\n",
    "    merged_records.extend(records)\n",
    "\n",
    "# Add the sequences to the seqs_of_records_with_undefined_seqs dictionary\n",
    "for record in merged_records:\n",
    "    seqs_of_records_with_undefined_seqs[record.id] = record.seq\n",
    "\n",
    "len(seqs_of_records_with_undefined_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to filter the intron hits to keep only those for which we can find reliable boundaries.\n",
    "In order to do so, we look for features labeled as intron in the corresponding GenBank entries that overlap by a minimum overlap threshold with the cmsearch hit.\n",
    "If there are no such features, we look for features with a location of type CompoundLocation (those shown as join(...)) and look for a gap between exons that overlaps with the cmsearch hit.\n",
    "We perform the search in both the direct and the complementary strand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_threshold = 50\n",
    "intron_boundaries_genbank_infernal_NT_search = {}\n",
    "rows_with_undefined_seqs = []\n",
    "\n",
    "for index, row in tqdm(infernal_NT_search_all_hits.iterrows(), total=infernal_NT_search_all_hits.shape[0]):\n",
    "    infernal_intron_start = int(row[\"seq from\"])\n",
    "    infernal_intron_end = int(row[\"seq to\"])\n",
    "    infernal_boundaries = (infernal_intron_start, infernal_intron_end)\n",
    "    infernal_hit_strand = row[\"strand\"]\n",
    "    genbank_ID = row[\"target name\"]\n",
    "    if genbank_ID == 'MT229979.1':\n",
    "        # skip this iteration since this seems to be a strange record that was removed from GenBank\n",
    "        continue\n",
    "    genbank_record = infernal_NT_search_genbank_recs[genbank_ID]\n",
    "    if genbank_record.seq.defined:\n",
    "        genbank_sequence_raw = genbank_record.seq\n",
    "    else:\n",
    "        genbank_sequence_raw = seqs_of_records_with_undefined_seqs[genbank_ID]\n",
    "    genbank_sequence = genbank_sequence_raw.replace('U', 'T')\n",
    "    genbank_features = genbank_record.features \n",
    "    new_intron_boundaries = None\n",
    "    source = None\n",
    "    candidate_reliable_introns = []\n",
    "    if(infernal_intron_start < infernal_intron_end):\n",
    "        infernal_intron_range = range(infernal_intron_start, infernal_intron_end)\n",
    "        for feature in genbank_features:\n",
    "            if feature.type == 'intron':\n",
    "            # Check for 'intron' features\n",
    "                if isinstance(feature.location, CompoundLocation):\n",
    "                    for i in range(len(feature.location.parts) - 1):\n",
    "                        start_first_intron = feature.location.parts[i].start\n",
    "                        end_first_intron = feature.location.parts[i].end\n",
    "                        candidate_intron_range = range(int(start_first_intron), int(end_first_intron))\n",
    "                        overlap = len(set(infernal_intron_range) & set(candidate_intron_range))\n",
    "                        if overlap >= overlap_threshold and candidate_intron_range[0] >= 1 and candidate_intron_range[1] < (len(genbank_sequence) - 1):\n",
    "                            source = 'intronCompoundLocation'\n",
    "                            new_intron_start = start_first_intron - 1\n",
    "                            new_intron_end = end_first_intron\n",
    "                            candidate_intron_sequence = genbank_sequence[new_intron_start:(new_intron_end + 1)]\n",
    "                            candidate_reliable_introns.append((genbank_ID, new_intron_start, new_intron_end, candidate_intron_sequence, overlap, source, infernal_intron_start, infernal_intron_end))\n",
    "                elif isinstance(feature.location.start, ExactPosition) and isinstance(feature.location.end, ExactPosition):\n",
    "                    candidate_intron_range = range(int(feature.location.start), int(feature.location.end))\n",
    "                    overlap = len(set(infernal_intron_range) & set(candidate_intron_range))\n",
    "                    if overlap >= overlap_threshold and candidate_intron_range[0] >= 1 and candidate_intron_range[1] < (len(genbank_sequence) - 1):\n",
    "                        source = 'intron'\n",
    "                        new_intron_start = int(feature.location.start) - 1\n",
    "                        new_intron_end = int(feature.location.end)\n",
    "                        candidate_intron_sequence = genbank_sequence[new_intron_start:(new_intron_end + 1)]\n",
    "                        candidate_reliable_introns.append((genbank_ID, new_intron_start, new_intron_end, candidate_intron_sequence, overlap, source, infernal_intron_start, infernal_intron_end))\n",
    "            elif isinstance(feature.location, CompoundLocation):\n",
    "            # Check for 'compound location' features\n",
    "                for i in range(len(feature.location.parts) - 1):\n",
    "                    # Get the end of the first part and the start of the second part\n",
    "                    end_first_part = feature.location.parts[i].end\n",
    "                    start_second_part = feature.location.parts[i+1].start\n",
    "                    candidate_intron_range = range(int(end_first_part), int(start_second_part))\n",
    "                    overlap = len(set(infernal_intron_range) & set(candidate_intron_range))\n",
    "                    if overlap >= overlap_threshold and start_second_part - end_first_part < 3000:\n",
    "                        source = 'compoundLocation'\n",
    "                        new_intron_start = end_first_part - 1\n",
    "                        new_intron_end = start_second_part\n",
    "                        candidate_intron_sequence = genbank_sequence[new_intron_start:(new_intron_end + 1)]\n",
    "                        candidate_reliable_introns.append((genbank_ID, new_intron_start, new_intron_end, candidate_intron_sequence, overlap, source, infernal_intron_start, infernal_intron_end))\n",
    "    else:\n",
    "        infernal_intron_range = range(infernal_intron_start, infernal_intron_end, -1)\n",
    "        for feature in genbank_features:\n",
    "            if feature.type == 'intron' and feature.location.strand == -1:\n",
    "            # Check for 'intron' features\n",
    "                if isinstance(feature.location.start, ExactPosition) and isinstance(feature.location.end, ExactPosition):\n",
    "                    candidate_intron_range = range(int(feature.location.end), int(feature.location.start), -1)\n",
    "                    overlap = len(set(infernal_intron_range) & set(candidate_intron_range))\n",
    "                    # It seems that ALWAYS int(feature.location.end) > int(feature.location.start) in these cases of features of type intron in strand -1\n",
    "                    if overlap >= overlap_threshold and candidate_intron_range[1] >= 1 and candidate_intron_range[0] < (len(genbank_sequence) - 1): #note indexes for accession on the range are reversed\n",
    "                        source = 'intron_reverseStrand'\n",
    "                        new_intron_start = int(feature.location.start) -1\n",
    "                        new_intron_end = int(feature.location.end)\n",
    "                        candidate_intron_sequence = genbank_sequence[new_intron_start:(new_intron_end + 1)]\n",
    "                        # Reverse complement the candidate_intron_sequence\n",
    "                        reverse_complement_sequence = Seq(candidate_intron_sequence).reverse_complement()\n",
    "                        candidate_reliable_introns.append((genbank_ID, new_intron_start, new_intron_end, reverse_complement_sequence, overlap, source, infernal_intron_start, infernal_intron_end))\n",
    "            elif isinstance(feature.location, CompoundLocation) and feature.location.strand == -1:\n",
    "                for i in range(len(feature.location.parts) - 1):\n",
    "                    start_first_part = feature.location.parts[i].start\n",
    "                    end_second_part = feature.location.parts[i+1].end\n",
    "                    candidate_intron_range = range(int(start_first_part), int(end_second_part), -1)\n",
    "                    overlap = len(set(infernal_intron_range) & set(candidate_intron_range))\n",
    "                    if overlap >= overlap_threshold and start_first_part - end_second_part < 3000:\n",
    "                        source = 'compoundLocation_reverseStrand'\n",
    "                        new_intron_start = end_second_part\n",
    "                        new_intron_end = start_first_part\n",
    "                        candidate_intron_sequence = genbank_sequence[new_intron_start:(new_intron_end + 1)]\n",
    "                        reverse_complement_sequence = Seq(candidate_intron_sequence).reverse_complement()\n",
    "                        candidate_reliable_introns.append((genbank_ID, new_intron_start, new_intron_end, reverse_complement_sequence, overlap, source, infernal_intron_start, infernal_intron_end))\n",
    "    if len(candidate_reliable_introns) >= 1:\n",
    "        if(len(candidate_reliable_introns) == 1):\n",
    "            selected_intron = candidate_reliable_introns[0]\n",
    "        else:\n",
    "            # Sort the candidate introns by overlap in descending order\n",
    "            candidate_reliable_introns.sort(key=lambda x: x[4], reverse=True)\n",
    "            # Get the highest overlap value\n",
    "            highest_overlap = candidate_reliable_introns[0][4]\n",
    "            # Filter the candidate introns with the highest overlap value\n",
    "            highest_overlap_introns = [intron for intron in candidate_reliable_introns if intron[4] == highest_overlap]\n",
    "            # Check if all the sequences are the same\n",
    "            sequences = [intron[3] for intron in highest_overlap_introns]\n",
    "            if len(set(sequences)) == 1:\n",
    "                selected_intron = highest_overlap_introns[0]\n",
    "            else:\n",
    "                # Check which introns have 'T' as their first letter\n",
    "                introns_with_T = [intron for intron in highest_overlap_introns if intron[3][0] == 'T']\n",
    "                if len(introns_with_T) > 0:\n",
    "                    selected_intron = introns_with_T[0]\n",
    "                else:\n",
    "                    selected_intron = highest_overlap_introns[0]\n",
    "    else:\n",
    "        selected_intron = None\n",
    "    if genbank_ID in intron_boundaries_genbank_infernal_NT_search:\n",
    "        if selected_intron is not None:\n",
    "            # then we have to append the selected intron to the list of introns\n",
    "            # check if the value is a list or a single value\n",
    "            if isinstance(intron_boundaries_genbank_infernal_NT_search[genbank_ID], list):\n",
    "                intron_boundaries_genbank_infernal_NT_search[genbank_ID].append(selected_intron)\n",
    "            else:\n",
    "                intron_boundaries_genbank_infernal_NT_search[genbank_ID] = [intron_boundaries_genbank_infernal_NT_search[genbank_ID], selected_intron]\n",
    "    else:\n",
    "        intron_boundaries_genbank_infernal_NT_search[genbank_ID] = selected_intron\n",
    "\n",
    "intron_boundaries_genbank_infernal_NT_search_found = {key: value for key, value in intron_boundaries_genbank_infernal_NT_search.items() if value is not None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select only introns that start with T (keeping in mind that the first nucleotide of each extracted intron is the last nucleotide of the exon), and we add the sequence length to each intron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT = {}\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found.items():\n",
    "    if isinstance(value, list):\n",
    "        # Filter out introns that don't start with a \"T\"\n",
    "        good_introns = [intron for intron in value if intron is not None and intron[3].startswith('T')]\n",
    "        if good_introns:\n",
    "            # If there's only one good intron, store it as a tuple instead of a list\n",
    "            if len(good_introns) == 1:\n",
    "                intron_boundaries_genbank_infernal_NT_search_found_startsWithT[key] = good_introns[0]\n",
    "            else:\n",
    "                intron_boundaries_genbank_infernal_NT_search_found_startsWithT[key] = good_introns\n",
    "    else:\n",
    "        # If the intron doesn't start with a \"T\", skip it\n",
    "        if not value[3].startswith('T'):\n",
    "            continue\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT[key] = value\n",
    "\n",
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths = {}\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found_startsWithT.items():\n",
    "    if isinstance(value, list):\n",
    "        new_value = []\n",
    "        for intron in value:\n",
    "            intron_length = len(intron[3])\n",
    "            new_intron = intron + (intron_length,)\n",
    "            new_value.append(new_intron)\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths[key] = new_value\n",
    "    else:\n",
    "        intron_length = len(value[3])\n",
    "        new_value = value + (intron_length,)\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths[key] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, there seems to be a single abnormal occurrence of an intron too long due to it having a compound feature annotated with a gap of nearly 100000 nucleotides. We simply remove this instance with a very high max length threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the maximum length\n",
    "max_length = 10000\n",
    "\n",
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs = {}\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths.items():\n",
    "    if isinstance(value, list):\n",
    "        # Filter out introns that are too long\n",
    "        good_introns = [intron for intron in value if intron[-1] <= max_length]\n",
    "        if good_introns:\n",
    "            # If there's only one good intron, store it as a tuple instead of a list\n",
    "            if len(good_introns) == 1:\n",
    "                intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs[key] = good_introns[0]\n",
    "            else:\n",
    "                intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs[key] = good_introns\n",
    "    else:\n",
    "        # If the intron is too long, skip it\n",
    "        if value[-1] > max_length:\n",
    "            continue\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the structure that we have is a dictionary where each key is a GenBank entry that contains at least 1 group I intron with reliable boundaries. If there is a single such intron, the value is a tuple describing the intron. If there are multiple, the value is a list of such tuples. We now want to extend each intron tuple with additional metadata: organism name, nucleic acid molecular type, taxonomy ID and subcellular location (organelle). We can extract these from the source feature of each GenBank entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend = {}\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs.items():\n",
    "    # Extract the additional data from the 'source' feature\n",
    "    source_feature = next(feature for feature in infernal_NT_search_genbank_recs[key].features if feature.type == 'source')\n",
    "    organism = source_feature.qualifiers.get('organism', [''])[0]\n",
    "    mol_type = source_feature.qualifiers.get('mol_type', [''])[0]\n",
    "    db_xref = source_feature.qualifiers.get('db_xref', [''])\n",
    "    # db_xref might be a list, so we want to take the element that contains the substring taxon\n",
    "    if isinstance(db_xref, list):\n",
    "        db_xref = next(xref for xref in db_xref if 'taxon' in xref)\n",
    "    organelle = source_feature.qualifiers.get('organelle', [''])[0]\n",
    "    if isinstance(value, list):\n",
    "        new_value = []\n",
    "        for intron in value:\n",
    "            new_intron = intron + (organism, mol_type, db_xref, organelle)\n",
    "            new_value.append(new_intron)\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend[key] = new_value\n",
    "    else:\n",
    "        new_value = value + (organism, mol_type, db_xref, organelle)\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend[key] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now flatten the dictionary so that each entry has a tuple as value. When required, we append _i to the keys with more than 1 intron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened = {}\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend.items():\n",
    "    if isinstance(value, list):\n",
    "        for i, intron in enumerate(value, start=1):\n",
    "            new_key = f\"{key}_{i}\"\n",
    "            intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened[new_key] = intron\n",
    "    else:\n",
    "        intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the 42459 group I introns with reliable boundaries. We can save the dictionary for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened.pkl', 'wb') as f:\n",
    "    pickle.dump(intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened.pkl', 'rb') as f:\n",
    "    intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each value in the dictionary of introns is a tuple describing the intron. We convert these to dictionaries to make clearer what each element is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict = {}\n",
    "\n",
    "for key, value in intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened.items():\n",
    "    genbank_id, start_position, end_position, sequence, overlap, boundariesSource, infernal_hit_start_position, infernal_hit_end_position, length, organism, molecule_type, tax_id, organelle = value\n",
    "    if organelle == \"mitochondrion\":\n",
    "        organelle = \"mitochondria\"\n",
    "    entry_dict = {\n",
    "        'GenBankID': genbank_id,\n",
    "        'startPosition': start_position,\n",
    "        'endPosition': end_position,\n",
    "        'sequence': sequence,\n",
    "        'overlapWithInfernalHit': overlap,\n",
    "        'boundariesSource': boundariesSource,\n",
    "        'length': length,\n",
    "        'organism': organism,\n",
    "        'moleculeType': molecule_type,\n",
    "        'taxID': tax_id,\n",
    "        'organelle': organelle,\n",
    "        'infernalHitStartPosition': infernal_hit_start_position,\n",
    "        'infernalHitEndPosition': infernal_hit_end_position\n",
    "    }\n",
    "    intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict[key] = entry_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through manual inspection of entries with strange taxonomy, we found 3 outliers that require manual fixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tFirstly, there is a case where authors seem to have by mistake assigned taxid of Olea gastropod genus,\n",
    "# whereas it should be the olive Olea as indicated by the organism name. So we are going to fix it manually\n",
    "# the corrected data are taken from Table S1 of the corresponding paper\n",
    "MT560017_1_entry = intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict['MT560017.1']\n",
    "MT560017_1_entry[\"organism\"] = \"Olea sp. POC544315\"\n",
    "MT560017_1_entry[\"taxID\"] = \"taxon:2813885\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we need to fix 2 entries that are annotated as artificial DNA because they were deposited as clones into vectors\n",
    "# First entry for K03428.1\n",
    "K03428_1_entry = intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict['K03428.1']\n",
    "K03428_1_entry[\"organism\"] = \"Tetrahymena thermophila\"\n",
    "K03428_1_entry[\"taxID\"] = \"taxon:5911\"\n",
    "\n",
    "# and then entry JN563930.1\n",
    "JN563930_1_entry = intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict['JN563930.1']\n",
    "JN563930_1_entry[\"organism\"] = \"Nicotiana undulata\"\n",
    "JN563930_1_entry[\"taxID\"] = \"taxon:118713\"\n",
    "JN563930_1_entry[\"organelle\"] = \"plastid:chloroplast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the triplets\n",
    "triplets = [(key, value['GenBankID'], value['startPosition'], value['endPosition']) for key, value in intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict.items()]\n",
    "\n",
    "# Group the triplets\n",
    "groups = defaultdict(list)\n",
    "for key, genbank_id, start_position, end_position in triplets:\n",
    "    groups[(genbank_id, start_position, end_position)].append(key)\n",
    "\n",
    "group_lengths = {key: len(value) for key, value in groups.items()}\n",
    "\n",
    "groups_with_one_element = {key: value for key, value in groups.items() if len(value) == 1}\n",
    "groups_with_more_than_one_element = {key: value for key, value in groups.items() if len(value) > 1}\n",
    "\n",
    "intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict_noDuplicates = {}\n",
    "keys_groups_with_one_element = [key for sublist in groups_with_one_element.values() for key in sublist]\n",
    "\n",
    "for key in keys_groups_with_one_element:\n",
    "    intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict_noDuplicates[key] = intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict[key]\n",
    "\n",
    "for triple, keys in groups_with_more_than_one_element.items():\n",
    "    subdictionary = {key: intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict[key] for key in keys}\n",
    "    infernal_start_positions = [value['infernalHitStartPosition'] for value in subdictionary.values()]\n",
    "    infernal_end_positions = [value['infernalHitEndPosition'] for value in subdictionary.values()]\n",
    "    infernal_boundaries = list(zip(infernal_start_positions, infernal_end_positions))\n",
    "    infernal_boundaries_unique = list(set(infernal_boundaries))\n",
    "    infernal_start_positions_unique, infernal_end_positions_unique = zip(*infernal_boundaries_unique)\n",
    "    entry_to_add = subdictionary[keys[0]]\n",
    "    entry_to_add[\"infernalHitStartPosition\"] = infernal_start_positions_unique\n",
    "    entry_to_add[\"infernalHitEndPosition\"] = infernal_end_positions_unique\n",
    "    intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict_noDuplicates[keys[0]] = entry_to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to classify the introns into organism types based on the taxonomy ID. We use a local copy of NCBI's taxonomy database for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi = NCBITaxa()\n",
    "# Update the local database\n",
    "ncbi.update_taxonomy_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_intron(key_value):\n",
    "    ncbi = NCBITaxa()\n",
    "    key, value = key_value\n",
    "    taxid_number = value.split(\":\")[1]\n",
    "    recovery_successful = False\n",
    "    updated_taxid = False\n",
    "    try:\n",
    "        lineage = ncbi.get_lineage(taxid_number)\n",
    "        recovery_successful = True\n",
    "    except Exception as e:\n",
    "        # then we are going to try to do the organism search from organisn name\n",
    "        # first we get the organism name from the flattened dictionary using the same key\n",
    "        organism_name = intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict_noDuplicates[key]['organism']\n",
    "        # then if the organism name contains the substring \"aff. \", we remove it\n",
    "        if \"aff. \" in organism_name:\n",
    "            organism_name = organism_name.replace(\"aff. \", \"\")\n",
    "        # then we are going to search for the organism name in the NCBI taxonomy database\n",
    "        taxid_number = ncbi.get_name_translator([organism_name])\n",
    "        taxid_number = taxid_number[organism_name][0]\n",
    "        updated_taxid = True\n",
    "        # and then we try with this new taxid_number, using error handling in case it is again not found\n",
    "        try:\n",
    "            lineage = ncbi.get_lineage(taxid_number)\n",
    "            recovery_successful = True\n",
    "        except Exception as e:\n",
    "            print(f\"No result found for organism name: {organism_name}. Error: {e}\")\n",
    "            category = \"other\"\n",
    "\n",
    "    if recovery_successful:\n",
    "        names = ncbi.get_taxid_translator(lineage)\n",
    "        names_values = names.values()\n",
    "        lineages_dict[key] = names_values\n",
    "    else:\n",
    "        # this should not happen for any key\n",
    "        lineages_dict[key] = None\n",
    "        print(f\"No result found for organism name: {organism_name}. Error: {e}\")\n",
    "        category = \"other\"\n",
    "    if \"Bacteria\" in names_values:\n",
    "        category = \"bacteria\"\n",
    "    elif 'Viruses' in names_values:\n",
    "        category = \"virus\"\n",
    "    elif \"Eukaryota\" in names_values:\n",
    "        if \"Viridiplantae\" in names_values or 'Diphylleia' in names_values:\n",
    "            category = \"plants\"\n",
    "        elif \"Mollusca\" in names_values:\n",
    "            category = \"molluscs\"\n",
    "        elif \"Fungi\" in names_values:\n",
    "            category = \"fungi\"\n",
    "        elif \"Ciliophora\" in names_values:\n",
    "            category = \"ciliates\"\n",
    "        elif \"Oomycota\" in names_values:\n",
    "            category = \"oomycetes\"\n",
    "        elif 'Acanthamoeba' in names_values or 'Amoebidium' in names_values or 'Dictyostelia' in names_values or 'Dictyostelium' in names_values or 'Heterostelium pallidum' in names_values or 'Myxogastria' in names_values or 'Physariida' in names_values or any(\"amoeba\" in element for element in names_values) or 'Amoebozoa' in names_values or 'Nuclearia' in names_values:\n",
    "            category = \"amoebae\"\n",
    "        elif 'Bacillariophyta' in names_values:\n",
    "            category = \"diatoms\"\n",
    "        elif 'Chlorarachniophyceae' in names_values:\n",
    "            category = \"green algae\"\n",
    "        elif 'Choanoflagellata' in names_values:\n",
    "            category = \"choanoflagellates\"\n",
    "        elif 'Cryptophyceae' in names_values:\n",
    "            category = \"cryptophytes\"\n",
    "        elif 'Cyanophora' in names_values:\n",
    "            category = \"glaucophytes\"\n",
    "        elif 'Euglenida' in names_values:\n",
    "            category = \"euglenids\"\n",
    "        elif 'Eustigmatophyceae' in names_values:\n",
    "            category = \"eustigmatophytes\"\n",
    "        elif 'Heterolobosea' in names_values:\n",
    "            category = \"percolozoa\"\n",
    "        elif 'Heteromitidae' in names_values:\n",
    "            category = \"cercomonads\"\n",
    "        elif 'Phaeophyceae' in names_values or 'Schizocladia' in names_values:\n",
    "            category = \"brown algae\"\n",
    "        elif 'Plasmodiophorida' in names_values:\n",
    "            category = \"plasmodiophores\"\n",
    "        elif 'Porifera' in names_values:\n",
    "            category = \"sponges\"\n",
    "        elif 'Rhodophyta' in names_values:\n",
    "            category = \"red algae\"\n",
    "        elif 'Xanthophyceae' in names_values:\n",
    "            category = \"yellow-green algae\"\n",
    "        elif 'Hexacorallia' in names_values:\n",
    "            category = \"corals\"\n",
    "        elif 'Insecta' in names_values:\n",
    "            category = \"insects\"\n",
    "        elif 'Vertebrata' in names_values:\n",
    "            category = \"vertebrates\"\n",
    "        elif 'Cercozoa' in names_values:\n",
    "            category = \"cercozoans\"\n",
    "        elif 'Centroplasthelida' in names_values:\n",
    "            category = \"centrohelids\"\n",
    "        elif 'Placozoa' in names_values:\n",
    "            category = \"placozoans\"\n",
    "        else:\n",
    "            category = \"other\"\n",
    "        # note that many of \n",
    "    else:\n",
    "        category = \"other\"\n",
    "    entry_to_update = intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict_noDuplicates[key]\n",
    "    extended_entry = entry_to_update.copy()\n",
    "    extended_entry[\"organismType\"] = category\n",
    "    extended_entry[\"lineage\"] = list(names_values)\n",
    "    if updated_taxid:\n",
    "        extended_entry[\"taxID\"] = taxid_number\n",
    "    return key, extended_entry\n",
    "\n",
    "classified_introns = {}\n",
    "lineages_dict = {}\n",
    "# Extract all taxon IDs\n",
    "taxon_ids = {key: entry['taxID'] for key, entry in intron_boundaries_genbank_infernal_NT_search_found_startsWithT_plus_lengths_removeTooLongs_orgExtend_flattened_dict_noDuplicates.items() if entry['taxID']}\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future_to_key = {executor.submit(classify_intron, item): item[0] for item in taxon_ids.items()}\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_to_key)):\n",
    "        key = future_to_key[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (key, exc))\n",
    "        else:\n",
    "            classified_introns[result[0]] = result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save this dictionary with categories of organisms as well. It also contains the entire taxonomic lineages of each organism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classified_introns.pkl', 'wb') as f:\n",
    "    pickle.dump(classified_introns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classified_introns.pkl', 'rb') as f:\n",
    "    classified_introns = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a barplot with the number of introns in each organism group. For this plot, we group all protists other than amoebae under category \"protists\". We group corals and sponges together also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the organism types\n",
    "organism_types = [entry['organismType'] for entry in classified_introns.values()]\n",
    "\n",
    "# Count the occurrences of each organism type\n",
    "counts = Counter(organism_types)\n",
    "\n",
    "# Create a new dictionary where we group the categories for other protists\n",
    "grouped_counts = {'plants': 0, 'fungi': 0, 'amoebae': 0, 'virus': 0, 'bacteria': 0, 'corals/sponges': 0, 'other protists': 0}\n",
    "for organism_type, count in counts.items():\n",
    "    if organism_type in grouped_counts:\n",
    "        grouped_counts[organism_type] += count\n",
    "    elif organism_type in ['corals', 'sponges']:\n",
    "        grouped_counts['corals/sponges'] += count\n",
    "    else:\n",
    "        grouped_counts['other protists'] += count\n",
    "\n",
    "# Sort the dictionary by the frequencies, except 'other protists' which should be last\n",
    "sorted_counts = {k: v for k, v in sorted(grouped_counts.items(), key=lambda item: (-item[1], item[0] == 'other protists'))}\n",
    "\n",
    "# Create the barplot\n",
    "\n",
    "plt.rc('font', family='Arial', size=9)\n",
    "plt.figure(figsize=(5.2/2.54, 5.2/2.54))\n",
    "\n",
    "plt.bar(sorted_counts.keys(), sorted_counts.values())\n",
    "plt.xlabel('Organism Type')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('Figure1b.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a similar barplot but excluding plants and fungi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary excluding 'plants' and 'fungi'\n",
    "filtered_counts = {k: v for k, v in counts.items() if k not in ['plants', 'fungi']}\n",
    "\n",
    "# Create the barplot\n",
    "plt.rc('font', family='Arial', size=8)\n",
    "plt.figure(figsize=(10/2.54, 5.2/2.54))\n",
    "\n",
    "plt.bar(filtered_counts.keys(), filtered_counts.values())\n",
    "plt.xlabel('Organism Type')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('Figure1c.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to identify putative homing endonucleases in the introns. We first extract all possible ORFs of a minimum length in all reading frames, taking into consideration that translation to protein should be done with the appropriate genetic code taking into account organism and subcellular location. The correct genetic code also defines with which codons a candidate ORF can start and end. First, we will get the correct translation tables from NCBI taxonomies and parse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ncbi_taxonomy_translation_tables(taxid):\n",
    "    requestUrl = \"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&id=\" + taxid\n",
    "    response = urllib.request.urlopen(requestUrl)\n",
    "    parsedResponse = BeautifulSoup(response, 'html.parser')\n",
    "    a_tags = parsedResponse.find_all('a')\n",
    "    genetic_codes = [(tag.previous_sibling + tag.text) for tag in a_tags if 'Translation table' in tag.text]\n",
    "    return genetic_codes\n",
    "\n",
    "translation_tables_from_NCBI_taxonomy = {}\n",
    "\n",
    "for key, value in tqdm(classified_introns.items()):\n",
    "    taxid = value[\"taxID\"].split(\":\")[1]\n",
    "    genetic_codes = get_ncbi_taxonomy_translation_tables(taxid)\n",
    "    translation_tables_from_NCBI_taxonomy[key] = genetic_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translation_tables_from_NCBI_taxonomy.pkl', 'wb') as f:\n",
    "    pickle.dump(translation_tables_from_NCBI_taxonomy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translation_tables_from_NCBI_taxonomy.pkl', 'rb') as f:\n",
    "    translation_tables_from_NCBI_taxonomy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_tables_from_NCBI_taxonomy_parsed = {}\n",
    "\n",
    "for key, value in translation_tables_from_NCBI_taxonomy.items():\n",
    "    # initialize a dictionary with keys general, mitochondrial, plastidic\n",
    "    translation_tables_from_NCBI_taxonomy_parsed[key] = {\"general\": None, \"mitochondrial\": None, \"plastidic\": None}\n",
    "    for item in value:\n",
    "        code_category, translation_table = item.split(\":\")\n",
    "        genetic_code_number = re.search(r'Translation table (\\d+)', translation_table).group(1)\n",
    "        if code_category == \"Genetic code\":\n",
    "            translation_tables_from_NCBI_taxonomy_parsed[key][\"general\"] = int(genetic_code_number)\n",
    "        elif code_category == \"Mitochondrial genetic code\":\n",
    "            translation_tables_from_NCBI_taxonomy_parsed[key][\"mitochondrial\"] = int(genetic_code_number)\n",
    "        elif code_category == \"Plastid genetic code\":\n",
    "            translation_tables_from_NCBI_taxonomy_parsed[key][\"plastidic\"] = int(genetic_code_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio.Data import CodonTable\n",
    "\n",
    "def find_orfs(sequence, frame, genetic_code, min_length):\n",
    "    start_codons = CodonTable.unambiguous_dna_by_id[genetic_code].start_codons\n",
    "    if frame < 3:\n",
    "        sequence_frame = sequence[frame:]\n",
    "    else:\n",
    "        sequence_frame = str(Seq(sequence).reverse_complement())[frame-3:]\n",
    "    protein = str(Seq(sequence_frame).translate(table=genetic_code, to_stop=False))\n",
    "    orfs = []\n",
    "    for i in range(0, len(sequence_frame), 3):\n",
    "        if sequence_frame[i:i+3] in start_codons:\n",
    "            protein_start = i // 3\n",
    "            for j in range(protein_start, len(protein)):\n",
    "                if protein[j] == \"*\":\n",
    "                    orf = protein[protein_start:j]\n",
    "                    if len(orf) >= min_length:\n",
    "                        orfs.append({\n",
    "                            'sequence': orf,\n",
    "                            'startInIntronSeq': frame + i,\n",
    "                            'endInIntronSeq': frame + i + len(orf) * 3,\n",
    "                            'geneticCode': genetic_code,\n",
    "                            'readingFrame': frame\n",
    "                        })\n",
    "                    break\n",
    "    # Remove ORFs that are fully contained within another ORF\n",
    "    orfs = [orf1 for orf1 in orfs if not any(orf2['startInIntronSeq'] <= orf1['startInIntronSeq'] and orf2['endInIntronSeq'] >= orf1['endInIntronSeq'] for orf2 in orfs if orf2 != orf1)]\n",
    "    return orfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = 120\n",
    "orf_dict_all_genetic_codes = {}\n",
    "all_possible_genetic_codes = [1,2,3,4,5,6,9,10,11,12,13,14,15,16,21,22,23,24,25,26,27,28,29,30,31,33]\n",
    "\n",
    "def get_intron_orfs(item):\n",
    "    key, value = item\n",
    "    orf_dict = {}\n",
    "    for genetic_code in all_possible_genetic_codes:\n",
    "        orf_dict[genetic_code] = {}\n",
    "        for frame in range(6):\n",
    "            orf_dict[genetic_code][frame] = find_orfs(value['sequence'], frame, genetic_code, min_length)\n",
    "    return key, orf_dict\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    orf_dict_all_genetic_codes = dict(executor.map(get_intron_orfs, classified_introns.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('orf_dict_all_genetic_codes.pkl', 'rb') as f:\n",
    "    orf_dict_all_genetic_codes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('orf_dict_all_genetic_codes.pkl', 'wb') as f:\n",
    "    pickle.dump(orf_dict_all_genetic_codes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"orfs_allIntrons_all_genetic_codes.fasta\", \"w\") as output_handle:\n",
    "    for key, value in orf_dict_all_genetic_codes.items():\n",
    "        for genetic_code, frames in value.items():\n",
    "            for frame, orfs in frames.items():\n",
    "                for i, orf in enumerate(orfs):\n",
    "                    sequence = Seq(orf['sequence'])\n",
    "                    record = SeqRecord(sequence, id=f\"{key}_allPossibleTranslations_geneticCode_{genetic_code}_frame_{frame}_orf_{i}\", description=\"\")\n",
    "                    SeqIO.write(record, output_handle, \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a run_interproscan.sh file that will be executed to run InterProScan on ORFs found on all introns with any genetic code. The file should be something like this:\n",
    "    \n",
    "```bash\n",
    "    #!/bin/bash\n",
    "    /path/to/interproscan.sh -i orfs_allIntrons_all_genetic_codes.fasta -f json -o orfs_allIntrons_all_genetic_codes.json -cpu 128 -dp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the shell script\n",
    "command = [\"bash\", \"run_interproscan.sh\"]\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "if process.returncode != 0:\n",
    "    print(f\"InterProScan failed with error message:\\n{stderr.decode()}\")\n",
    "else:\n",
    "    print(\"InterProScan for all introns with all genetic codes completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the InterProScan results and add them to a dictionary\n",
    "interpro_results_allGeneticCodes = {}\n",
    "with open('orfs_allIntrons_all_genetic_codes.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for protein in data['results']:\n",
    "        if len(protein[\"matches\"]) == 0:\n",
    "            # then move on to next\n",
    "            continue\n",
    "        # first lets get the key (intronID), which will be the capture group containing all characters from the beginning until the substring _allPossible\n",
    "        # but we need to iterate over all elements in protein['xref'], since sequences are indexed by sequence\n",
    "        for i in range(len(protein['xref'])):\n",
    "            key = re.match(r\"(.*)_allPossibleTranslations\", protein['xref'][i]['id']).group(1)\n",
    "            id_parts = protein['xref'][0]['id'].split('_')\n",
    "            orf_index = int(id_parts[-1])\n",
    "            frame = int(id_parts[-3])\n",
    "            geneticCode = int(id_parts[-5])\n",
    "            if key not in interpro_results_allGeneticCodes:\n",
    "                interpro_results_allGeneticCodes[key] = {}\n",
    "            if geneticCode not in interpro_results_allGeneticCodes[key]:\n",
    "                interpro_results_allGeneticCodes[key][geneticCode] = {}\n",
    "            if frame not in interpro_results_allGeneticCodes[key][geneticCode]:\n",
    "                interpro_results_allGeneticCodes[key][geneticCode][frame] = {}\n",
    "            interpro_results_allGeneticCodes[key][geneticCode][frame][orf_index] = protein['matches']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make a new dictionary where we only keep introns for which an ORF likely to be a homing endonuclease was found. We also add the InterProScan hits for these cases. It is possible that some introns might contain multiple homing endonucleases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_substrings(d):\n",
    "    # Check if the current dictionary contains the substrings\n",
    "    if any(\"endonuc\" in value.lower() or \"homing\" in value.lower() or \"nuclease\" in value.lower() \n",
    "           for value in d.values() if isinstance(value, str)):\n",
    "        return True\n",
    "    # Recursively check the nested dictionaries\n",
    "    return any(check_substrings(value) for value in d.values() if isinstance(value, dict))\n",
    "\n",
    "# Make a deep copy of the orf_dict_all_genetic_codes dictionary\n",
    "homingEndonucleases_dict = copy.deepcopy(orf_dict_all_genetic_codes)\n",
    "discarded_ORFs_dict = copy.deepcopy(orf_dict_all_genetic_codes)\n",
    "\n",
    "# Iterate over each intron in the copied dictionary\n",
    "for intron, genetic_codes in list(homingEndonucleases_dict.items()):  # Use list to allow modifying the dictionary during iteration\n",
    "    # For each intron, iterate over each frame\n",
    "    for genetic_code, frames in list(genetic_codes.items()):  # Use list to allow modifying the dictionary during iteration\n",
    "        for frame, orfs in list(frames.items()):  # Use list to allow modifying the dictionary during iteration\n",
    "            # Create a new list that only includes the ORFs you want to keep\n",
    "            new_orfs = []\n",
    "            # For each frame, iterate over each ORF\n",
    "            for i, orf in enumerate(orfs):\n",
    "                # Check if the ORF has any InterProScan hits\n",
    "                if intron in interpro_results_allGeneticCodes and genetic_code in interpro_results_allGeneticCodes[intron] and frame in interpro_results_allGeneticCodes[intron][genetic_code] and i in interpro_results_allGeneticCodes[intron][genetic_code][frame]:\n",
    "                    hits = interpro_results_allGeneticCodes[intron][genetic_code][frame][i]\n",
    "                    # Check if any hit contains the substrings \"endonuc\", \"homing\", or \"nuclease\", ignoring case\n",
    "                    if any(check_substrings(hit) for hit in hits):\n",
    "                        # Append the list of InterProScan hits to the ORF's dictionary\n",
    "                        orf['interProScanHits'] = hits\n",
    "                        # Add the ORF to the new list\n",
    "                        new_orfs.append(orf)\n",
    "                    else:\n",
    "                        # Append the list of InterProScan hits to the ORF's dictionary in discarded_ORFs_dict\n",
    "                        discarded_ORFs_dict[intron][genetic_code][frame][i]['interProScanHits'] = hits\n",
    "                else:\n",
    "                    # Treat the ORF as if there were hits but they did not contain any of the specified substrings\n",
    "                    orf['interProScanHits'] = []\n",
    "            # Replace the old list of ORFs with the new list\n",
    "            frames[frame] = new_orfs\n",
    "        # Remove the genetic code if all its frames are empty\n",
    "        if all(not orfs for orfs in frames.values()):\n",
    "            del homingEndonucleases_dict[intron][genetic_code]\n",
    "    # remove the intron if all its genetic codes are empty\n",
    "    if all(not frames for frames in genetic_codes.values()):\n",
    "        del homingEndonucleases_dict[intron]\n",
    "\n",
    "# Iterate over each intron in the discarded_ORFs_dict dictionary\n",
    "for intron, genetic_codes in list(discarded_ORFs_dict.items()):  # Use list to allow modifying the dictionary during iteration\n",
    "    # For each intron, iterate over each frame\n",
    "    for genetic_code, frames in list(genetic_codes.items()):\n",
    "        for frame, orfs in list(frames.items()):  # Use list to allow modifying the dictionary during iteration\n",
    "            # Remove the frame if all its ORFs have no InterProScan hits\n",
    "            if all('interProScanHits' not in orf or not orf['interProScanHits'] for orf in orfs):\n",
    "                del frames[frame]\n",
    "        # Remove the genetic code if all its frames are empty\n",
    "        if all(not orfs for orfs in frames.values()):\n",
    "            del discarded_ORFs_dict[intron][genetic_code]\n",
    "    # remove the intron if all its genetic codes are empty\n",
    "    if all(not frames for frames in genetic_codes.values()):\n",
    "        del discarded_ORFs_dict[intron]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homingEndonucleases_dict_reshaped = {}\n",
    "\n",
    "# Iterate over each intron in homingEndonucleases_dict\n",
    "for intron, genetic_codes in homingEndonucleases_dict.items():\n",
    "    # Initialize an empty dict for the current intron. each key will be a genetic code\n",
    "    homingEndonucleases_dict_reshaped[intron] = {}\n",
    "    for genetic_code, frames in genetic_codes.items():\n",
    "        # initialize this element of the dict to an empty list for the current genetic code of the current intron\n",
    "        homingEndonucleases_dict_reshaped[intron][genetic_code] = []\n",
    "        # For each intron, iterate over each frame of each genetic code\n",
    "        for frame, orfs in frames.items():\n",
    "            # For each frame, iterate over each ORF\n",
    "            for orf in orfs:\n",
    "                # Create a new dictionary that is a copy of the ORF's dictionary\n",
    "                orf_copy = copy.deepcopy(orf)\n",
    "                # Add a new key to the new dictionary with the name 'frame' and the value of the current frame\n",
    "                orf_copy['frame'] = frame\n",
    "                # Append the new dictionary to the list of ORFs for the current intron\n",
    "                homingEndonucleases_dict_reshaped[intron][genetic_code].append(orf_copy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also save a dictionary of hits that matched some other protein family different than endonucleases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded_ORFs_dict_noDisorderHits = {}\n",
    "\n",
    "for intron, genetic_codes in discarded_ORFs_dict.items():  \n",
    "    for genetic_code, frames in genetic_codes.items(): \n",
    "        for frame, orfs in frames.items(): \n",
    "            for i, orf in enumerate(orfs):\n",
    "                if not 'interProScanHits' in orf.keys():\n",
    "                    continue\n",
    "                hits = orf['interProScanHits']\n",
    "                new_hits = []\n",
    "                for j, hit in enumerate(hits):\n",
    "                    if hit[\"signature\"][\"name\"] is None or not (\"disorder\" in hit[\"signature\"][\"name\"].lower() or \"coil\" in hit[\"signature\"][\"name\"].lower()):\n",
    "                        new_hits.append(hit)\n",
    "                if new_hits:\n",
    "                    # we make a copy of orf where we replace the interProScanHits with the new list of hits\n",
    "                    new_orf = orf.copy()\n",
    "                    new_orf['interProScanHits'] = new_hits\n",
    "                    if intron not in discarded_ORFs_dict_noDisorderHits:\n",
    "                        discarded_ORFs_dict_noDisorderHits[intron] = {}\n",
    "                    if genetic_code not in discarded_ORFs_dict_noDisorderHits[intron]:\n",
    "                        discarded_ORFs_dict_noDisorderHits[intron][genetic_code] = {}\n",
    "                    if frame not in discarded_ORFs_dict_noDisorderHits[intron][genetic_code]:\n",
    "                        discarded_ORFs_dict_noDisorderHits[intron][genetic_code][frame] = {}\n",
    "                    discarded_ORFs_dict_noDisorderHits[intron][genetic_code][frame][i] = new_orf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save these dictionaries for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('homingEndonucleases_dict_reshaped_allGeneticCodes.pkl', 'wb') as f:\n",
    "    pickle.dump(homingEndonucleases_dict_reshaped, f)\n",
    "with open('discarded_ORFs_dict_noDisorderHits.pkl', 'wb') as f:\n",
    "    pickle.dump(discarded_ORFs_dict_noDisorderHits, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('homingEndonucleases_dict_reshaped_allGeneticCodes.pkl', 'rb') as f:\n",
    "    homingEndonucleases_dict_reshaped = pickle.load(f)\n",
    "with open('discarded_ORFs_dict_noDisorderHits.pkl', 'rb') as f:\n",
    "    discarded_ORFs_dict_noDisorderHits = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets count the total number of homing endonucleases identified. In order to do so, we sum the length of each list stored as the value of each key of homingEndonucleases_dict_reshaped\n",
    "\n",
    "total_homing_endonucleases = sum(len(value) for value in homingEndonucleases_dict_reshaped.values())\n",
    "\n",
    "print(f\"Total number of putative homing endonucleases: {total_homing_endonucleases}\")\n",
    "\n",
    "print(f\"These are located in a total of {len(homingEndonucleases_dict_reshaped)} introns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a dictionary where each key is a number of endonucleases per intron and each value is a list with the introns that have that number of endonucleases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endonuclease_counter = Counter(len(value) for value in homingEndonucleases_dict_reshaped.values())\n",
    "\n",
    "print(endonuclease_counter)\n",
    "\n",
    "introns_by_number_endonucleases = {f\"{count} endonuclease\" if count == 1 else f\"{count} endonucleases\": [] for count in endonuclease_counter.keys()}\n",
    "for key, value in homingEndonucleases_dict_reshaped.items():\n",
    "    count = len(value)\n",
    "    introns_by_number_endonucleases[f\"{count} endonuclease\" if count == 1 else f\"{count} endonucleases\"].append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the key for the intron with 4 detected endonucleases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(introns_by_number_endonucleases[\"4 endonucleases\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look for introns that had a large difference between the assigned start/end point and the start/end point of the Infernal hit, and that do not contain a homing endonuclease identified through InterProScan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classified_introns_bigDifferenceStartOrEndPoints = {}\n",
    "\n",
    "distance_threshold = 1000\n",
    "\n",
    "for key, value in classified_introns.items():\n",
    "    #check if element boundariesSource contains the substring reverse\n",
    "    if isinstance(value[\"infernalHitEndPosition\"], int):\n",
    "        if \"reverse\" in value['boundariesSource']:\n",
    "            # check that value['infernalHitStartPosition'] is not a tuple\n",
    "            if abs(value['infernalHitStartPosition'] - value['endPosition']) > distance_threshold or abs(value['infernalHitEndPosition'] - value['startPosition']) > distance_threshold:\n",
    "                classified_introns_bigDifferenceStartOrEndPoints[key] = value\n",
    "        else:\n",
    "            if abs(value['infernalHitStartPosition'] - value['startPosition']) > distance_threshold or abs(value['infernalHitEndPosition'] - value['endPosition']) > distance_threshold:\n",
    "                classified_introns_bigDifferenceStartOrEndPoints[key] = value\n",
    "    else:\n",
    "        anyInfernalBoundariesNoBigDifference = True\n",
    "        list_of_boundaries = list(zip(value['infernalHitStartPosition'], value['infernalHitEndPosition']))\n",
    "        # initialize a list of the same length as list_of_boundaries\n",
    "        infernalBoundariesNoBigDifference = [True] * len(list_of_boundaries)\n",
    "        for i in range(len(list_of_boundaries)):\n",
    "            start, end = list_of_boundaries[i]\n",
    "            if \"reverse\" in value['boundariesSource']:\n",
    "                if abs(start - value['endPosition']) > distance_threshold or abs(end - value['startPosition']) > distance_threshold:\n",
    "                    infernalBoundariesNoBigDifference[i] = False\n",
    "            else:\n",
    "                if abs(start - value['startPosition']) > distance_threshold or abs(end - value['endPosition']) > distance_threshold:\n",
    "                    infernalBoundariesNoBigDifference[i] = False\n",
    "        if not any(infernalBoundariesNoBigDifference):\n",
    "            classified_introns_bigDifferenceStartOrEndPoints[key] = value\n",
    "\n",
    "# get the keys of classified_introns_bigDifferenceStartOrEndPoints that are not present in homingEndonucleases_dict_reshaped\n",
    "\n",
    "introns_bigDifference_not_in_homingEndonucleases_dict_reshaped = set(classified_introns_bigDifferenceStartOrEndPoints.keys()) - set(homingEndonucleases_dict_reshaped.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's get the candidate ORFs for these introns (with all possible genetic codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now lets get the entries of orf_dict_all_genetic_codes corresponding only to the keys in keys_not_in_homingEndonucleases_dict_reshaped\n",
    "\n",
    "orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE = {key: value for key, value in orf_dict_all_genetic_codes.items() if key in introns_bigDifference_not_in_homingEndonucleases_dict_reshaped}\n",
    "\n",
    "orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE_flattened = {}\n",
    "\n",
    "for key, value in orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE.items():\n",
    "    for genetic_code, frames in value.items():\n",
    "        for frame, orfs in frames.items():\n",
    "            for i, orf in enumerate(orfs):\n",
    "                orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE_flattened[key + f\"_geneticCode_{genetic_code}_frame_{frame}_orfNumber_{i}\"] = orf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to identify additional homing endonucleases by running DELTA-BLAST on these ORFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_deltablast(sequence, id):\n",
    "    # Write the sequence to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "        sequence_record = SeqRecord(Seq(sequence), id=id)\n",
    "        SeqIO.write(sequence_record, temp.name, \"fasta\")\n",
    "\n",
    "        # Create a unique temporary file for the BLAST results\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp_blast_results:\n",
    "            blastn_cline = NcbideltablastCommandline(query=temp.name, db=\"/apps/unit/BioinfoUgrp/DB/blastDB/ncbi/2022-07-nr/nr\", outfmt=5, out=temp_blast_results.name, rpsdb=\"/flash/WolfU/rafa/intron_g1/cdd_delta_db/cdd_delta\", cmd=\"/apps/unit/BioinfoUgrp/Other/ncbi-blast/2.13.0+/bin/deltablast\")\n",
    "            stdout, stderr = blastn_cline()\n",
    "\n",
    "        # Parse the BLAST results\n",
    "        result_handle = open(temp_blast_results.name)\n",
    "        blast_record = NCBIXML.read(result_handle)\n",
    "        return id, blast_record\n",
    "\n",
    "# Step 3: Perform BLAST search for each ORF of each frame in each genetic code for all introns\n",
    "deltablast_results_orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE = {}\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = {executor.submit(run_deltablast, orf['sequence'], key): key for key, orf in orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE_flattened.items()}\n",
    "    progress = tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"BLASTs completed\")\n",
    "    for future in progress:\n",
    "        key = futures[future]\n",
    "        try:\n",
    "            _, blast_record = future.result()\n",
    "            deltablast_results_orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE[key] = blast_record\n",
    "        except Exception as exc:\n",
    "            print(f'{key} generated an exception: {exc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deltablast_results_orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE.pkl', 'wb') as f:\n",
    "    pickle.dump(deltablast_results_orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deltablast_results_orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE.pkl', 'rb') as f:\n",
    "    deltablast_results_orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make a data frame combining all identified homing endonucleases. First, let's make a dataframe only with HEGs from InterProScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets initialize a new pandas dataframe with columns intronID,GenBankID,startPositionInIntron,endPositionInIntron,putativeSequence,geneticCode,readingFrame,annotationOrigin\n",
    "homingEndonucleasesDF_interProScan_raw = pd.DataFrame(columns=['intronID', 'GenBankID', 'startPositionInIntron', 'endPositionInIntron', 'putativeSequence', 'geneticCode', 'readingFrame', 'annotationOrigin'])\n",
    "\n",
    "def check_substrings(d):\n",
    "    # Check if the current dictionary contains the substrings\n",
    "    if any(\"endonuc\" in value.lower() or \"homing\" in value.lower() or \"nuclease\" in value.lower() \n",
    "           for value in d.values() if isinstance(value, str)):\n",
    "        return True\n",
    "    # Recursively check the nested dictionaries\n",
    "    return any(check_substrings(value) for value in d.values() if isinstance(value, dict)) \n",
    "\n",
    "for intron in homingEndonucleases_dict_reshaped:\n",
    "    intron_subcellular_location = classified_introns[intron][\"organelle\"]\n",
    "    if \"plastid\" in intron_subcellular_location:\n",
    "        expected_genetic_code = translation_tables_from_NCBI_taxonomy_parsed[intron][\"plastidic\"]\n",
    "        other_genetic_codes = [translation_tables_from_NCBI_taxonomy_parsed[intron][\"general\"], translation_tables_from_NCBI_taxonomy_parsed[intron][\"mitochondrial\"]]\n",
    "    elif \"mitochondr\" in intron_subcellular_location:\n",
    "        expected_genetic_code = translation_tables_from_NCBI_taxonomy_parsed[intron][\"mitochondrial\"]\n",
    "        other_genetic_codes = [translation_tables_from_NCBI_taxonomy_parsed[intron][\"general\"], translation_tables_from_NCBI_taxonomy_parsed[intron][\"plastidic\"]]\n",
    "    else:\n",
    "        expected_genetic_code = translation_tables_from_NCBI_taxonomy_parsed[intron][\"general\"]\n",
    "        other_genetic_codes = [translation_tables_from_NCBI_taxonomy_parsed[intron][\"mitochondrial\"], translation_tables_from_NCBI_taxonomy_parsed[intron][\"plastidic\"]]\n",
    "    interproscan_hegs_current_intron = homingEndonucleases_dict_reshaped[intron]\n",
    "    for genetic_code in interproscan_hegs_current_intron:\n",
    "        hegs_current_genetic_code = interproscan_hegs_current_intron[genetic_code]\n",
    "        for heg in hegs_current_genetic_code:\n",
    "            if(genetic_code == expected_genetic_code):\n",
    "                annotationOrigin = \"InterProScan hit with expected genetic code for organism and subcellular location\"\n",
    "            elif(genetic_code in other_genetic_codes):\n",
    "                annotationOrigin = \"InterProScan hit with expected genetic code for organism but not subcellular location\"\n",
    "            else:\n",
    "                annotationOrigin = \"InterProScan hit with unexpected genetic code\"\n",
    "            new_row = pd.DataFrame({\n",
    "                'intronID': [intron],\n",
    "                'GenBankID': [classified_introns[intron][\"GenBankID\"]],\n",
    "                'startPositionInIntron': [heg['startInIntronSeq']],\n",
    "                'endPositionInIntron': [heg['endInIntronSeq']],\n",
    "                'putativeSequence': [heg['sequence']],\n",
    "                'geneticCode': [genetic_code],\n",
    "                'readingFrame': [heg['frame']],\n",
    "                'annotationOrigin': [annotationOrigin]\n",
    "            })\n",
    "            homingEndonucleasesDF_interProScan_raw = pd.concat([homingEndonucleasesDF_interProScan_raw, new_row], ignore_index=True)\n",
    "\n",
    "introns_with_hegs_from_interproscan = homingEndonucleasesDF_interProScan_raw['intronID'].unique()\n",
    "\n",
    "homingEndonucleasesDF_InterProScan = pd.DataFrame(columns=['intronID', 'GenBankID', 'startPositionInIntron', 'endPositionInIntron', 'putativeSequence', 'geneticCode', 'readingFrame', 'annotationOrigin'])\n",
    "\n",
    "for intron in introns_with_hegs_from_interproscan:\n",
    "    subDF_hegs = homingEndonucleasesDF_interProScan_raw[homingEndonucleasesDF_interProScan_raw['intronID'] == intron]\n",
    "    hegs_expected_genetic_code = subDF_hegs[subDF_hegs['annotationOrigin'] == \"InterProScan hit with expected genetic code for organism and subcellular location\"]\n",
    "    hegs_other_organelle_genetic_code = subDF_hegs[subDF_hegs['annotationOrigin'] == \"InterProScan hit with expected genetic code for organism but not subcellular location\"]\n",
    "    hegs_unexpected_genetic_code = subDF_hegs[subDF_hegs['annotationOrigin'] == \"InterProScan hit with unexpected genetic code\"]\n",
    "    # for hegs_other_organelle_genetic_code, we need to cluster the entries that have the same readingFrame, startPositionInIntron and endPositionInIntron \n",
    "    hegs_other_organelle_genetic_code_mergedSameSeq = hegs_other_organelle_genetic_code.groupby(['readingFrame', 'startPositionInIntron', 'endPositionInIntron']).agg({\n",
    "        'intronID': 'first',\n",
    "        'GenBankID': 'first',\n",
    "        'putativeSequence': 'first',\n",
    "        'geneticCode': lambda x: \",\".join(map(str, x)),\n",
    "        'annotationOrigin': 'first'\n",
    "    }).reset_index()\n",
    "    # we do the same for hegs_unexpected_genetic_code\n",
    "    hegs_unexpected_genetic_code_mergedSameSeq = hegs_unexpected_genetic_code.groupby(['readingFrame', 'startPositionInIntron', 'endPositionInIntron']).agg({\n",
    "        'intronID': 'first',\n",
    "        'GenBankID': 'first',\n",
    "        'putativeSequence': 'first',\n",
    "        'geneticCode': lambda x: \",\".join(map(str, x)),\n",
    "        'annotationOrigin': 'first'\n",
    "    }).reset_index()\n",
    "    # then add to the new DF\n",
    "    homingEndonucleasesDF_InterProScan = pd.concat([homingEndonucleasesDF_InterProScan, hegs_expected_genetic_code, hegs_other_organelle_genetic_code_mergedSameSeq, hegs_unexpected_genetic_code_mergedSameSeq], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make a dataframe with HEGs from DELTA-BLAST. When using DELTA-BLAST hits to identify HEGs, we will only consider hits with an e-value lower than 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homingEndonucleasesDF_DeltaBlast_raw = pd.DataFrame(columns=['intronID', 'GenBankID', 'startPositionInIntron', 'endPositionInIntron', 'putativeSequence', 'geneticCode', 'readingFrame', 'annotationOrigin'])\n",
    "e_value_threshold = 0.1\n",
    "\n",
    "for key in deltablast_results_orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE:\n",
    "    deltablast_hit = deltablast_results_orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE[key]\n",
    "    intronID = re.match(r\"(.*)_geneticCode\", key).group(1)\n",
    "    geneticCode = int(re.search(r\"geneticCode_(\\d+)_frame\", key).group(1))\n",
    "    frame = int(re.search(r\"frame_(\\d+)_orfNumber\", key).group(1))\n",
    "    orfNumber = int(re.search(r\"orfNumber_(\\d+)\", key).group(1))\n",
    "    intron_subcellular_location = classified_introns[intron][\"organelle\"]\n",
    "    if \"plastid\" in intron_subcellular_location:\n",
    "        expected_genetic_code = translation_tables_from_NCBI_taxonomy_parsed[intron][\"plastidic\"]\n",
    "        other_genetic_codes = [translation_tables_from_NCBI_taxonomy_parsed[intron][\"general\"], translation_tables_from_NCBI_taxonomy_parsed[intron][\"mitochondrial\"]]\n",
    "    elif \"mitochondr\" in intron_subcellular_location:\n",
    "        expected_genetic_code = translation_tables_from_NCBI_taxonomy_parsed[intron][\"mitochondrial\"]\n",
    "        other_genetic_codes = [translation_tables_from_NCBI_taxonomy_parsed[intron][\"general\"], translation_tables_from_NCBI_taxonomy_parsed[intron][\"plastidic\"]]\n",
    "    else:\n",
    "        expected_genetic_code = translation_tables_from_NCBI_taxonomy_parsed[intron][\"general\"]\n",
    "        other_genetic_codes = [translation_tables_from_NCBI_taxonomy_parsed[intron][\"mitochondrial\"], translation_tables_from_NCBI_taxonomy_parsed[intron][\"plastidic\"]]\n",
    "    if(genetic_code == expected_genetic_code):\n",
    "        annotationOrigin = \"DeltaBLAST hit with expected genetic code for organism and subcellular location\"\n",
    "    elif(genetic_code in other_genetic_codes):\n",
    "        annotationOrigin = \"DeltaBLAST hit with expected genetic code for organism but not subcellular location\"\n",
    "    else:\n",
    "        annotationOrigin = \"DeltaBLAST hit with unexpected genetic code\"\n",
    "    length_alignment_largest_hit = 0\n",
    "    for alignment in deltablast_hit.alignments:\n",
    "        title = alignment.title.lower()\n",
    "        length_alignment = alignment.length\n",
    "        # we want to extract the hit ID, which will be the substring contained between | characters in alignment.hit_id\n",
    "        hit_id = re.search(r\"\\|(.*)\\|\", alignment.hit_id).group(1)\n",
    "        if (\"endonuc\" in title or \"homing\" in title or \"nuclease\" in title):\n",
    "            for hsp in alignment.hsps:\n",
    "                if hsp.expect < e_value_threshold and length_alignment > length_alignment_largest_hit:\n",
    "                    length_alignment_largest_hit = length_alignment\n",
    "                    annotationOrigin_with_hitID = annotationOrigin + f\" with RefSeq ID {hit_id}\"\n",
    "                    break\n",
    "    if(length_alignment_largest_hit > 0):\n",
    "        new_row = pd.DataFrame({\n",
    "            'intronID': [intronID],\n",
    "            'GenBankID': [classified_introns[intronID][\"GenBankID\"]],\n",
    "            'startPositionInIntron': [orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE_flattened[key]['startInIntronSeq']],\n",
    "            'endPositionInIntron': [orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE_flattened[key]['endInIntronSeq']],\n",
    "            'putativeSequence': [orf_dict_all_genetic_codes_intronsBigDiff_and_no_HE_flattened[key]['sequence']],\n",
    "            'geneticCode': [geneticCode],\n",
    "            'readingFrame': [frame],\n",
    "            'annotationOrigin': [annotationOrigin_with_hitID]\n",
    "        })\n",
    "        homingEndonucleasesDF_DeltaBlast_raw = pd.concat([homingEndonucleasesDF_DeltaBlast_raw, new_row], ignore_index=True)\n",
    "\n",
    "introns_with_hegs_from_deltablast = homingEndonucleasesDF_DeltaBlast_raw['intronID'].unique()\n",
    "\n",
    "homingEndonucleasesDF_DeltaBLAST = pd.DataFrame(columns=['intronID', 'GenBankID', 'startPositionInIntron', 'endPositionInIntron', 'putativeSequence', 'geneticCode', 'readingFrame', 'annotationOrigin'])\n",
    "\n",
    "for intron in introns_with_hegs_from_deltablast:\n",
    "    subDF_hegs = homingEndonucleasesDF_DeltaBlast_raw[homingEndonucleasesDF_DeltaBlast_raw['intronID'] == intron]\n",
    "    hegs_expected_genetic_code = subDF_hegs[subDF_hegs['annotationOrigin'].str.startswith(\"DeltaBLAST hit with expected genetic code for organism and subcellular location\")]\n",
    "    hegs_other_organelle_genetic_code = subDF_hegs[subDF_hegs['annotationOrigin'].str.startswith(\"DeltaBLAST hit with expected genetic code for organism but not subcellular location\")]\n",
    "    hegs_unexpected_genetic_code = subDF_hegs[subDF_hegs['annotationOrigin'].str.startswith(\"DeltaBLAST hit with unexpected genetic code\")]\n",
    "    hegs_other_organelle_genetic_code_mergedSameSeq = hegs_other_organelle_genetic_code.groupby(['readingFrame', 'startPositionInIntron', 'endPositionInIntron']).agg({\n",
    "        'intronID': 'first',\n",
    "        'GenBankID': 'first',\n",
    "        'putativeSequence': 'first',\n",
    "        'geneticCode': lambda x: \",\".join(map(str, x)),\n",
    "        'annotationOrigin': 'first'\n",
    "    }).reset_index()\n",
    "    hegs_unexpected_genetic_code_mergedSameSeq = hegs_unexpected_genetic_code.groupby(['readingFrame', 'startPositionInIntron', 'endPositionInIntron']).agg({\n",
    "        'intronID': 'first',\n",
    "        'GenBankID': 'first',\n",
    "        'putativeSequence': 'first',\n",
    "        'geneticCode': lambda x: \",\".join(map(str, x)),\n",
    "        'annotationOrigin': 'first'\n",
    "    }).reset_index()\n",
    "    homingEndonucleasesDF_DeltaBLAST = pd.concat([homingEndonucleasesDF_DeltaBLAST, hegs_expected_genetic_code, hegs_other_organelle_genetic_code_mergedSameSeq, hegs_unexpected_genetic_code_mergedSameSeq], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homingEndonucleasesDF = pd.concat([homingEndonucleasesDF_InterProScan, homingEndonucleasesDF_DeltaBLAST], ignore_index=True)\n",
    "homingEndonucleasesDF.to_csv(\"homingEndonucleasesDF.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homingEndonucleasesDF = pd.read_csv(\"homingEndonucleasesDF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to compare the histogram of lengths for all introns and for introns where homing endonucleases were detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the sizes of the introns from the classified_introns dictionary\n",
    "all_intron_sizes = [min(len(intron[\"sequence\"]), 5000) for intron in classified_introns.values()]\n",
    "\n",
    "# Extract the sizes of the introns with HEGs\n",
    "introns_with_HEG  = homingEndonucleasesDF['intronID'].unique()\n",
    "homing_endonuclease_intron_sizes = [min(len(classified_introns[intron][\"sequence\"]), 5000) for intron in introns_with_HEG if intron in classified_introns]\n",
    "\n",
    "min_size = 0\n",
    "max_size = 3000\n",
    "\n",
    "# Create a histogram for the distribution of intron sizes of all introns\n",
    "plt.rc('font', family='Arial', size=8)\n",
    "plt.figure(figsize=(15/2.54, 7.5/2.54))\n",
    "\n",
    "plt.hist(all_intron_sizes, bins=60, alpha=0.5, color=\"blue\", range=(min_size, max_size), label='All introns', density=True)\n",
    "\n",
    "# Create a histogram for the distribution of intron sizes of homing endonuclease introns\n",
    "plt.hist(homing_endonuclease_intron_sizes, bins=60, alpha=0.5, color=\"red\", range=(min_size, max_size), label='Introns with homing endonucleases', density=True)\n",
    "\n",
    "plt.xlabel('Intron size')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Set xticks\n",
    "xticks = np.arange(min_size, max_size+1, 500)\n",
    "xticks = np.append(xticks, max_size)\n",
    "plt.xticks(xticks, [str(int(xtick)) if xtick < max_size else '>3000' for xtick in xticks])\n",
    "\n",
    "plt.xlim(min_size, max_size)\n",
    "\n",
    "plt.savefig('Figure1d.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to perform secondary structure predictions. First let's set up the arnie.conf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"eternafold: /path/to/EternaFold/src\" > arnie.conf\n",
    "!echo \"vienna_2: /path/to/ViennaRNA/bin\" >> arnie.conf\n",
    "!echo \"nupack: /path/to/nupack3/bin\" >> arnie.conf\n",
    "!echo \"contrafold_2: /path/to/contrafold-se/src\" >> arnie.conf\n",
    "!echo \"rnastructure: /path/to/RNAstructure/exe\" >> arnie.conf\n",
    "!echo \"rnasoft: /path/to/MultiRNAFold\" >> arnie.conf\n",
    "!echo \"linearfold: /path/to/LinearFold/bin\" >> arnie.conf\n",
    "!echo \"linearpartition: /path/to/LinearPartition/bin\" >> arnie.conf\n",
    "!echo \"spotrna: /path/to/SPOT-RNA\" >> arnie.conf\n",
    "!echo \"ipknot: /path/to/ipknot/bin\" >> arnie.conf\n",
    "!echo \"TMP: /path/to/tmp/folder/\" >> arnie.conf\n",
    "\n",
    "os.environ[\"ARNIEFILE\"] = f'/path/to/arnie.conf'\n",
    "os.environ[\"DATAPATH\"] = f'/path/to/RNAstructure/data_tables'\n",
    "\n",
    "import arnie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we run predictions of secondary structure and base-pairing probabilities for all introns with all available software. We start with mean free energy (MFE) secondary structure predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arnie.mfe import mfe\n",
    "\n",
    "symbol_mapping = {\n",
    "    'N': 'A',\n",
    "    'S': 'C',\n",
    "    'Y': 'C',\n",
    "    'B': 'C',\n",
    "    'R': 'A',\n",
    "    'D': 'A',\n",
    "    'W': 'A',\n",
    "    'K': 'G',\n",
    "    'V': 'A',\n",
    "    'H': 'A',\n",
    "    'M': 'A',\n",
    "    'T': 'U'\n",
    "}\n",
    "\n",
    "def process_intron(key, value):\n",
    "    sequence = value['sequence']\n",
    "    for non_standard, standard in symbol_mapping.items():\n",
    "        sequence = sequence.replace(non_standard, standard)\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        os.chdir(tmpdir)\n",
    "        try:\n",
    "            eternafold_MFE_structure = mfe(sequence, package='eternafold')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with eternafold: {e}\")\n",
    "            eternafold_MFE_structure = e\n",
    "        try:\n",
    "            vienna_MFE_structure = mfe(sequence, package='vienna_2', DEBUG=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with vienna_2: {e}\")\n",
    "            vienna_MFE_structure = e\n",
    "        try:\n",
    "            contrafold_MFE_structure = mfe(sequence, package='contrafold_2', viterbi=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with contrafold_2: {e}\")\n",
    "            contrafold_MFE_structure = e\n",
    "        try:\n",
    "            rnastructure_MFE_structure = mfe(sequence, package='rnastructure', pseudo=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with rnastructure: {e}\")\n",
    "            rnastructure_MFE_structure = e\n",
    "    return key, eternafold_MFE_structure, vienna_MFE_structure, contrafold_MFE_structure, rnastructure_MFE_structure\n",
    "\n",
    "eternafold_MFE_secondary_structures = {}\n",
    "vienna_MFE_secondary_structures = {}\n",
    "contrafold_MFE_secondary_structures = {}\n",
    "rnastructure_MFE_secondary_structures = {}\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_intron, key, value): key for key, value in classified_introns.items()}\n",
    "    pbar = tqdm(total=len(futures), desc=\"Processing introns\", dynamic_ncols=True)\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        key, eternafold_MFE_structure, vienna_MFE_structure, contrafold_MFE_structure, rnastructure_MFE_structure = future.result()\n",
    "        eternafold_MFE_secondary_structures[key] = eternafold_MFE_structure\n",
    "        vienna_MFE_secondary_structures[key] = vienna_MFE_structure\n",
    "        contrafold_MFE_secondary_structures[key] = contrafold_MFE_structure\n",
    "        rnastructure_MFE_secondary_structures[key] = rnastructure_MFE_structure\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "with open('results_secondary_structures_MFE.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'eternafold': eternafold_MFE_secondary_structures,\n",
    "        'vienna': vienna_MFE_secondary_structures,\n",
    "        'contrafold': contrafold_MFE_secondary_structures,\n",
    "        'rnastructure': rnastructure_MFE_secondary_structures\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can read back the MFE secondary structures\n",
    "\n",
    "with open('results_secondary_structures_MFE.pkl', 'rb') as f:\n",
    "    MFE_secondary_structures = pickle.load(f)\n",
    "\n",
    "eternafold_MFE_secondary_structures = MFE_secondary_structures['eternafold']\n",
    "vienna_MFE_secondary_structures = MFE_secondary_structures['vienna']\n",
    "contrafold_MFE_secondary_structures = MFE_secondary_structures['contrafold']\n",
    "rnastructure_MFE_secondary_structures = MFE_secondary_structures['rnastructure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to calculate base pair probability (BPP) matrixes for all introns with each package. Note this will take a long time, and require large amounts of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arnie.bpps import bpps\n",
    "\n",
    "symbol_mapping = {\n",
    "    'N': 'A',\n",
    "    'S': 'C',\n",
    "    'Y': 'C',\n",
    "    'B': 'C',\n",
    "    'R': 'A',\n",
    "    'D': 'A',\n",
    "    'W': 'A',\n",
    "    'K': 'G',\n",
    "    'V': 'A',\n",
    "    'H': 'A',\n",
    "    'M': 'A',\n",
    "    'T': 'U'\n",
    "}\n",
    "\n",
    "def process_intron_bpps(key, value):\n",
    "    sequence = str(value['sequence'])\n",
    "    for non_standard, standard in symbol_mapping.items():\n",
    "        sequence = sequence.replace(non_standard, standard)\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        os.chdir(tmpdir)\n",
    "        try:\n",
    "            eternafold_BPPS = bpps(sequence, package='eternafold')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with eternafold: {e}\")\n",
    "            eternafold_BPPS = e\n",
    "        try:\n",
    "            vienna_BPPS = bpps(sequence, package='vienna_2')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with vienna_2: {e}\")\n",
    "            vienna_BPPS = e\n",
    "        try:\n",
    "            contrafold_BPPS = bpps(sequence, package='contrafold_2')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with contrafold_2: {e}\")\n",
    "            contrafold_BPPS = e\n",
    "        try:\n",
    "            rnastructure_BPPS = bpps(sequence, package='rnastructure')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with rnastructure: {e}\")\n",
    "            rnastructure_BPPS = e\n",
    "        try:\n",
    "            rnasoft_BPPS = bpps(sequence, package='rnasoft')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with rnasoft: {e}\")\n",
    "            rnasoft_BPPS = e\n",
    "        try:\n",
    "            nupack_BPPS = bpps(sequence, package='nupack')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing intron {key} with nupack: {e}\")\n",
    "            nupack_BPPS = e\n",
    "    return key, eternafold_BPPS, vienna_BPPS, contrafold_BPPS, rnastructure_BPPS, rnasoft_BPPS, nupack_BPPS\n",
    "\n",
    "eternalfold_BPPS_matrixes = {}\n",
    "vienna_BPPS_matrixes = {}\n",
    "contrafold_BPPS_matrixes = {}\n",
    "rnastructure_BPPS_matrixes = {}\n",
    "rnasoft_BPPS_matrixes = {}\n",
    "nupack_BPPS_matrixes = {}\n",
    "\n",
    "eternalfold_BPPS_matrixes_errors = {}\n",
    "vienna_BPPS_matrixes_errors = {}\n",
    "contrafold_BPPS_matrixes_errors = {}\n",
    "rnastructure_BPPS_matrixes_errors = {}\n",
    "rnasoft_BPPS_matrixes_errors = {}\n",
    "nupack_BPPS_matrixes_errors = {}\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_intron_bpps, key, value): key for key, value in classified_introns.items()}\n",
    "    pbar = tqdm(total=len(futures), desc=\"Processing introns\", dynamic_ncols=True)\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        key, eternafold_BPPS, vienna_BPPS, contrafold_BPPS, rnastructure_BPPS, rnasoft_BPPS, nupack_BPPS = future.result()\n",
    "        if isinstance(eternafold_BPPS, Exception):\n",
    "            eternalfold_BPPS_matrixes_errors[key] = eternafold_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/eternafold/{key}_eternafold.npy\", \"wb\") as f:\n",
    "                np.save(f, eternafold_BPPS)\n",
    "        if isinstance(vienna_BPPS, Exception):\n",
    "            vienna_BPPS_matrixes_errors[key] = vienna_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/vienna/{key}_vienna.npy\", \"wb\") as f:\n",
    "                np.save(f, vienna_BPPS)\n",
    "        if isinstance(contrafold_BPPS, Exception):\n",
    "            contrafold_BPPS_matrixes_errors[key] = contrafold_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/contrafold/{key}_contrafold.npy\", \"wb\") as f:\n",
    "                np.save(f, contrafold_BPPS)\n",
    "        if isinstance(rnastructure_BPPS, Exception):\n",
    "            rnastructure_BPPS_matrixes_errors[key] = rnastructure_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/rnastructure/{key}_rnastructure.npy\", \"wb\") as f:\n",
    "                np.save(f, rnastructure_BPPS)\n",
    "        if isinstance(rnasoft_BPPS, Exception):\n",
    "            rnasoft_BPPS_matrixes_errors[key] = rnasoft_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/rnasoft/{key}_rnasoft.npy\", \"wb\") as f:\n",
    "                np.save(f, rnasoft_BPPS)\n",
    "        if isinstance(nupack_BPPS, Exception):\n",
    "            nupack_BPPS_matrixes_errors[key] = nupack_BPPS\n",
    "        else:\n",
    "            with open(f\"BPPS_all/nupack/{key}_nupack.npy\", \"wb\") as f:\n",
    "                np.save(f, nupack_BPPS)\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPPs run successfully with all software for all introns except for a set of 87 introns, which fail for RNAsoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_keys_rnastructure_BPPS_matrixes = list(rnastructure_BPPS_matrixes_errors.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will classify the introns into subtypes. First, we need to recreate the subgroup-specific CMs from the multiple sequence alignments (in Stockholm format). We first run cmbuild on the MSA and then run cmcalibrate on the resulting CM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtypes_MSAs_folder = \"/path/to/input/MSAs_subgroups\"\n",
    "output_CMs_folder = \"/path/to/output/CMs_subgroups\"\n",
    "cmbuild_path = \"/path/to/cmbuild/bin/cmbuild\"\n",
    "cmcalibrate_path = \"/path/to/cmcalibrate/bin/cmcalibrate\"\n",
    "num_cpus = 120\n",
    "\n",
    "# lets iterate over the different sto files in subtypes_MSAs_folder\n",
    "sto_files = glob.glob(subtypes_MSAs_folder + \"/*.sto\")\n",
    "for MSA_file in sto_files:\n",
    "    # Define the commands\n",
    "    output_cm_filename = os.path.basename(MSA_file).replace('.sto', '.cm')\n",
    "    cmbuild_command = f\"{cmbuild_path} {output_CMs_folder}/{output_cm_filename} {MSA_file}\"\n",
    "    cmcalibrate_command = f\"{cmcalibrate_path} --cpu {num_cpus} {output_CMs_folder}/{output_cm_filename}\"\n",
    "    # Run the commands\n",
    "    print(f\"Processing cmbuild for {MSA_file}\")\n",
    "    process = subprocess.Popen(cmbuild_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    print(f\"Processing cmcalibrate for {MSA_file}\")\n",
    "    process = subprocess.Popen(cmcalibrate_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we prepare the database of CMs against which the intron sequences will be run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to concatenate all the CMs into a single file\n",
    "\n",
    "output_cm_filename = \"/path/to/output/CMs_subgroups/all_subgroups.cm\"\n",
    "cm_files = glob.glob(output_CMs_folder + \"/*.cm\")\n",
    "cat_command = f\"cat {' '.join(cm_files)} > {output_cm_filename}\"\n",
    "process = subprocess.Popen(cat_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "# now we run cmpress on the concatenated CMs\n",
    "\n",
    "cmpress_path = \"/path/to/cmpress/bin/cmpress\"\n",
    "cmpress_command = f\"{cmpress_path} {output_cm_filename}\"\n",
    "process = subprocess.Popen(cmpress_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run cmscan of each intron against the database of CMs, and assign a subtype to each intron based on the CM that gives the best hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intron_sequences = [intron[\"sequence\"] for intron in classified_introns.values()]\n",
    "intron_sequences_RNA = [Seq(seq.replace(\"T\", \"U\")) for seq in intron_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 20\n",
    "\n",
    "cmscan_path = \"/path/to/cmscan/bin/cmscan\"\n",
    "\n",
    "cmscan_runs_path = \"/path/to/output/cmscan_runs\"\n",
    "\n",
    "def process_intron(args):\n",
    "    i, intron_seq, cmscan_path, cmscan_runs_path, output_cm_filename, num_cpus = args\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as fasta_file:\n",
    "        fasta_file.write(f\">intron_{list(classified_introns.keys())[i]}\\n{intron_seq}\\n\")\n",
    "        fasta_file_path = fasta_file.name\n",
    "    output_tbl_filename = f\"{cmscan_runs_path}/intron_{list(classified_introns.keys())[i]}_max.tbl\"\n",
    "    cmscan_command = f\"{cmscan_path} --cpu {num_cpus} --max --tblout {output_tbl_filename} {output_cm_filename} {fasta_file_path}\"\n",
    "    process = subprocess.Popen(cmscan_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    os.remove(fasta_file_path)\n",
    "\n",
    "# Create a ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    args = [(i, seq, cmscan_path, cmscan_runs_path, output_cm_filename, 5) for i, seq in enumerate(intron_sequences_RNA)]\n",
    "    list(tqdm(executor.map(process_intron, args), total=len(intron_sequences_RNA), desc=\"Processing introns\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cmscan_tbl_file(file_path):\n",
    "    column_names = [\"target name\", \"accession target\", \"query name\", \"accession query\", \"mdl\", \"mdl from\", \"mdl to\", \"seq from\", \"seq to\", \"strand\", \"trunc\", \"pass\", \"gc\", \"bias\", \"score\", \"E-value\", \"inc\", \"description of target\"]\n",
    "    df = pd.read_csv(file_path, comment='#', sep='\\s+', names = column_names, header=None)\n",
    "    return df\n",
    "\n",
    "df_best_hit_subtypes = pd.DataFrame(columns=[\"best hit subtype\", \"mdl from\", \"mdl to\", \"seq from\", \"seq to\", \"strand\", \"E-value\", \"intron ID\"])\n",
    "\n",
    "list_intron_keys = list(classified_introns.keys())\n",
    "\n",
    "for i, intron_seq in enumerate(intron_sequences_RNA):\n",
    "    tbl_file = f\"{cmscan_runs_path}/intron_{list(classified_introns.keys())[i]}_max.tbl\"\n",
    "    df = read_cmscan_tbl_file(tbl_file)\n",
    "    # some dataframes might have no rows. we need to provide None as the value for all columns in this case\n",
    "    if df.shape[0] == 0:\n",
    "        df_best_hit_subtypes.loc[i] = [None, None, None, None, None, None, None, list(classified_introns.keys())[i]]\n",
    "    else:\n",
    "        best_hit = df.iloc[0]\n",
    "        df_best_hit_subtypes.loc[i] = [best_hit[\"target name\"], best_hit[\"mdl from\"], best_hit[\"mdl to\"], best_hit[\"seq from\"], best_hit[\"seq to\"], best_hit[\"strand\"], best_hit[\"E-value\"], list_intron_keys[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_hit_subtypes.to_csv(\"best_hit_subtypes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_hit_subtypes = pd.read_csv(\"best_hit_subtypes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to assign secondary structure elements to each intron based on the structural elements found in the CM consensus secondary structure for the corresponding subtype. We first obtain alignments of each intron against the CM of the corresponding subtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of threads\n",
    "num_threads = 20\n",
    "\n",
    "cmalign_path = \"/path/to/cmalign/bin/cmalign\"\n",
    "\n",
    "intron_sequences = [intron[\"sequence\"] for intron in classified_introns.values()]\n",
    "intron_sequences_RNA = [Seq(seq.replace(\"T\", \"U\")) for seq in intron_sequences]\n",
    "\n",
    "best_subtype_alignment_runs_path = \"/path/to/output/best_subtype_cmalignments\"\n",
    "\n",
    "cms_subgroups_folder_path = output_CMs_folder\n",
    "\n",
    "def align_intron_to_best_subtype(args):\n",
    "    i, intron_seq, best_subtype, cmalign_path, best_subtype_alignment_runs_path, cms_subgroups_folder_path, num_cpus = args\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as fasta_file:\n",
    "        fasta_file.write(f\">intron_{list(classified_introns.keys())[i]}\\n{intron_seq}\\n\")\n",
    "        fasta_file_path = fasta_file.name\n",
    "    output_stockholm_filename = f\"{best_subtype_alignment_runs_path}/intron_{list(classified_introns.keys())[i]}_alignment_to_CM_subtype_{best_subtype}.tbl\"\n",
    "    best_subtype_cm_filename = f\"{cms_subgroups_folder_path}/{best_subtype}.cm\"\n",
    "    cmalign_command = f\"{cmalign_path} --cpu {num_cpus} --ileaved --mxsize 230000 --nonbanded -o {output_stockholm_filename} {best_subtype_cm_filename} {fasta_file_path}\"\n",
    "    process = subprocess.Popen(cmalign_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    os.remove(fasta_file_path)\n",
    "\n",
    "# Create a ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    args = [(i, seq, df_best_hit_subtypes.iloc[i,0], cmalign_path, best_subtype_alignment_runs_path, cms_subgroups_folder_path, 6) for i, seq in enumerate(intron_sequences_RNA)]\n",
    "    list(tqdm(executor.map(align_intron_to_best_subtype, args), total=len(intron_sequences_RNA), desc=\"Processing alignments of introns to best subtype CM\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then read the alignment for each intron and compare it with the annotation file of the corresponding subtype, which contains the annotation of structural elements for each position of the consensus sequence (provided as csv files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_consensus_subtypes = dict()\n",
    "\n",
    "path_annotated_subtype_consensus = \"/flash/YokobayashiU/lara/group_I_intron_database/CMs_subgroups\"\n",
    "\n",
    "for file in os.listdir(path_annotated_subtype_consensus):\n",
    "    if file.endswith(\"_annotated_consensus.csv\"):\n",
    "        subtype = file.split(\"_\")[0]\n",
    "        # the separator is ; in the csv files\n",
    "        df = pd.read_csv(f\"{path_annotated_subtype_consensus}/{file}\", sep=';')        \n",
    "        annotated_consensus_subtypes[subtype] = df\n",
    "\n",
    "# and now we iterate over the introns alignments and annotate each with structural elements based on the annotations of the consensus sequences\n",
    "\n",
    "def annotate_structural_elements(alignment_to_CM, annotated_consensus, subtype):\n",
    "    consensus_with_gaps = alignment_to_CM.column_annotations[\"reference_annotation\"]\n",
    "    consensus_ss_with_gaps = alignment_to_CM.column_annotations[\"secondary_structure\"]\n",
    "    aligned_sequence = str(alignment_to_CM[0].seq)\n",
    "    annotations = annotated_consensus['Group I intron structural element']\n",
    "    mapping = {}\n",
    "    ungapped_pos = 0\n",
    "    for gapped_pos, char in enumerate(consensus_with_gaps):\n",
    "        if char != '.' and char != '~':\n",
    "            mapping[gapped_pos] = ungapped_pos\n",
    "            ungapped_pos += 1\n",
    "    sequence_annotations = [\"-\"] * len(aligned_sequence)\n",
    "    consensus_SS_ungapped = []\n",
    "    for gapped_pos, char in enumerate(aligned_sequence):\n",
    "        if char != '-':\n",
    "            consensus_SS_ungapped.append(consensus_ss_with_gaps[gapped_pos])\n",
    "            ungapped_pos = mapping.get(gapped_pos)\n",
    "            if ungapped_pos is not None:\n",
    "                sequence_annotations[gapped_pos] = annotations[ungapped_pos] \n",
    "    # we remove from sequence_annotations the elements at positions for which the character in aligned_sequence is a -\n",
    "    sequence_annotations = [sequence_annotations[i] for i, char in enumerate(aligned_sequence) if char != '-']\n",
    "    # we collapse the annotations into a single string\n",
    "    sequence_annotations = \",\".join(sequence_annotations)\n",
    "    consensus_SS_ungapped = \"\".join(consensus_SS_ungapped)\n",
    "    return sequence_annotations,consensus_SS_ungapped\n",
    "\n",
    "best_subtype_alignment_runs_path = \"/flash/YokobayashiU/lara/group_I_intron_database/best_subtype_cmalignments\"\n",
    "\n",
    "dict_structural_elements = dict()\n",
    "dict_consensus_SS = dict()\n",
    "\n",
    "for intron in classified_introns.keys():\n",
    "    subtype = df_best_hit_subtypes[df_best_hit_subtypes['intron ID'] == intron].iloc[0,0]\n",
    "    alignment_file = f\"{best_subtype_alignment_runs_path}/intron_{intron}_alignment_to_CM_subtype_{subtype}.tbl\"\n",
    "    alignment = AlignIO.read(alignment_file, \"stockholm\")\n",
    "    annotated_consensus = annotated_consensus_subtypes[subtype]\n",
    "    dict_structural_elements[intron],dict_consensus_SS[intron] = annotate_structural_elements(alignment, annotated_consensus, subtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dict_structural_elements.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict_structural_elements, f)\n",
    "\n",
    "with open(\"dict_consensus_SS.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict_consensus_SS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dict_structural_elements.pkl\", \"rb\") as f:\n",
    "    dict_structural_elements = pickle.load(f)\n",
    "\n",
    "with open(\"dict_consensus_SS.pkl\", \"rb\") as f:\n",
    "    dict_consensus_SS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the preceding and following exonic context. Extracted intron sequences already contained the last base of the preceding exon and the first base of the following exon; these are included again in the exonic contexts. Up to 150 bases of each are extracted (if there are not enough bases available, extracted sequences are padded with \"-\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_introns_preceding_exons = {}\n",
    "classified_introns_following_exons = {}\n",
    "number_bases_preceding_exon = 150\n",
    "number_bases_following_exon = 150\n",
    "\n",
    "for key, value in tqdm(classified_introns.items(), total=len(classified_introns)):\n",
    "    start_position_intron = value['startPosition']\n",
    "    end_position_intron = value['endPosition'] + 1\n",
    "    genbankID = value['GenBankID']\n",
    "    genbank_record = infernal_NT_search_genbank_recs[genbankID]\n",
    "    if genbank_record.seq.defined:\n",
    "        genbank_sequence_raw = genbank_record.seq\n",
    "    else:\n",
    "        genbank_sequence_raw = seqs_of_records_with_undefined_seqs[genbankID]\n",
    "    genbank_sequence = genbank_sequence_raw.replace('U', 'T')\n",
    "    source = value['boundariesSource']\n",
    "    if(\"reverseStrand\" in source):\n",
    "        reverseStrand = True\n",
    "    else:\n",
    "        reverseStrand = False\n",
    "    if(reverseStrand):\n",
    "        following_exon_seq = genbank_sequence_raw[max((start_position_intron - number_bases_following_exon + 1), 0):(start_position_intron+1)].reverse_complement()\n",
    "        preceding_exon_seq = genbank_sequence_raw[(end_position_intron-1):(end_position_intron + number_bases_preceding_exon - 1)].reverse_complement()\n",
    "    else:\n",
    "        following_exon_seq = genbank_sequence_raw[(end_position_intron-1):(end_position_intron + number_bases_following_exon - 1)]\n",
    "        preceding_exon_seq = genbank_sequence_raw[max((start_position_intron - number_bases_preceding_exon + 1), 0):(start_position_intron+1)]\n",
    "    if(len(following_exon_seq) < number_bases_following_exon):\n",
    "        following_exon_seq = following_exon_seq + \"-\"*(number_bases_following_exon - len(following_exon_seq))\n",
    "    if(len(preceding_exon_seq) < number_bases_preceding_exon):\n",
    "        preceding_exon_seq = \"-\"*(number_bases_preceding_exon - len(preceding_exon_seq)) + preceding_exon_seq\n",
    "    classified_introns_preceding_exons[key] = preceding_exon_seq\n",
    "    classified_introns_following_exons[key] = following_exon_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open(\"classified_introns_preceding_exons.pkl\", \"wb\")) as f:\n",
    "    pickle.dump(classified_introns_preceding_exons, f)\n",
    "\n",
    "with(open(\"classified_introns_following_exons.pkl\", \"wb\")) as f:\n",
    "    pickle.dump(classified_introns_following_exons, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create the database files. First the main dataframe of all introns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for key, value in classified_introns.items():\n",
    "    genbankID = value['GenBankID']\n",
    "    startPosition = int(value['startPosition']) + 1\n",
    "    endPosition = int(value['endPosition']) + 1\n",
    "    intronSequence = str(value['sequence'])\n",
    "    intronLength = value['length']\n",
    "    intronSubtype = df_best_hit_subtypes[df_best_hit_subtypes['intron ID'] == key].iloc[0,0]\n",
    "    source = value['boundariesSource']\n",
    "    if \"reverseStrand\" in source:\n",
    "        strand = \"reverse\"\n",
    "    else:\n",
    "        strand = \"forward\"\n",
    "    organism = value['organism']\n",
    "    organismType = value['organismType']\n",
    "    subcellularLocation = value['organelle']\n",
    "    if subcellularLocation == '':\n",
    "        if organismType == \"bacteria\":\n",
    "            subcellularLocation = 'cytoplasm'\n",
    "        elif organismType == \"virus\":\n",
    "            subcellularLocation = 'virion'\n",
    "        else:\n",
    "            subcellularLocation = 'nucleus'\n",
    "    taxID = value['taxID']\n",
    "    if not isinstance(taxID, int):\n",
    "        taxID = taxID.split(\":\")[1]\n",
    "    taxID = str(taxID)\n",
    "    precedingExonSequence = str(classified_introns_preceding_exons[key])\n",
    "    followingExonSequence = str(classified_introns_following_exons[key])\n",
    "    eternafoldSecondaryStructure = eternafold_MFE_secondary_structures[key]\n",
    "    viennaSecondaryStructure = vienna_MFE_secondary_structures[key]\n",
    "    contrafoldSecondaryStructure = contrafold_MFE_secondary_structures[key]\n",
    "    rnastructureSecondaryStructure = rnastructure_MFE_secondary_structures[key]\n",
    "    consensusSecondaryStructure = dict_consensus_SS[key]\n",
    "    consensusStructuralElements = dict_structural_elements[key]\n",
    "    data.append([key, genbankID, startPosition, endPosition, intronSequence, intronLength, intronSubtype, strand, organism, organismType, subcellularLocation, taxID, precedingExonSequence, followingExonSequence, eternafoldSecondaryStructure, viennaSecondaryStructure, contrafoldSecondaryStructure, rnastructureSecondaryStructure, consensusSecondaryStructure, consensusStructuralElements])\n",
    "\n",
    "intronDatabaseDF = pd.DataFrame(data, columns=['intronID', 'GenBankID', 'startPosition', 'endPosition', 'intronSequence', 'intronLength', 'intronSubtype', 'strand', 'organism', 'organismType', 'subcellularLocation', 'organismTaxID', 'precedingExonSequence', 'followingExonSequence', 'eternafoldSecondaryStructure', 'viennaSecondaryStructure', 'contrafoldSecondaryStructure', 'rnastructureSecondaryStructure', 'consensusSecondaryStructure', 'consensusStructuralElements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intronDatabaseDF.to_csv(\"intronDatabase.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the column intronID is shared between database files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
